---
title: "Estimation"
subtitle: "POLI_SCI 403: Probability and Statistics"
format: 
  revealjs:
    slide-number: false
    progress: false
    code-overflow: wrap
    chalkboard: true
---

## Agenda

- Random variables and random sampling

- Estimation

- Lab

## Moving toward working with data

**Mean squared error around $\color{purple}c$**

$$
MSE = E[(X-\color{purple}c)^2]
$$

. . .

We said that $\color{purple} c =E[X]$ is the best predictor of $X$ because it would minimize MSE

. . .

But $E[X]$ is a theoretical quantity!

. . .

We can use data to **approximate** it

. . .

But how do we now if it is a *good* approximation?

## Ingredients for statistical inference

::: incremental
1. Estimand $\theta$

2. Estimator $\widehat \theta$
:::

## Ingredients for statistical inference

1. Estimand $\mu$

2. Estimator $\widehat \mu$

## Ingredients for statistical inference

1. Estimand $\mu$

2. Estimator $\widehat \mu$

3. Data $X$

## Ingredients for statistical inference

1. Estimand $\mu$

2. Estimator $\widehat \mu$

3. Data $X$

4. Estimate $\overline X$

## Ingredients for statistical inference

1. Estimand $\mu$

2. Estimator $\widehat \mu$

3. Data $X$

4. Estimate $\overline X$

5. Quantify uncertainty (next week)


. . .

::: {style="text-align: center"}

$X \rightarrow \overline{X} \rightarrow \widehat{\mu} \xrightarrow{\text{hopefully!}} \mu$

:::



## Plug-in principle

**Idea:** Use *sample quantities* to approximate *population parameters*

. . .

We can replace $\theta = T(F)$ with $\widehat \theta = T(\widehat{F})$ 

::: aside
$F$ is the (theoretical) CDF and $\widehat F$ is the *empirical* CDF
:::

## Example

If we have

$$
T_E(F) = E[X] = \int_{-\infty}^{\infty} xdF(x)
$$

. . .

We can *plug-in*

$$
T_E(\widehat F) = \widehat E[X] = \frac{1}{n} \sum_{i=1}^n x_i
$$

::: aside
See Example 3.3.4 in AM (p. 119)
:::

## Example

If we have

$$
T_E(F) = E[X] = \int_{-\infty}^{\infty} xdF(x)
$$


We can *plug-in*

$$
T_E(\widehat F) = \widehat E[X] = \overline X
$$

. . .

But how can we tell if any of this is legit?

::: aside
See Example 3.3.4 in AM (p. 119)
:::

# Random sampling



## Independent, identically distributed

We have random variables $X_1, X_2, \ldots, X_n$

. . .

with their respective CDFs $F_1, F_2, \ldots, F_n$

. . .

A collection of **random variables** is i.i.d. if they are:

. . .

1. Mutually independent

2. Identically distributed

. . .

Meaning we are taking $n$ draws of the same (identical) random variable independently from each other?

## Illustration

**Question from the [2022 General Social Survey](https://gss.norc.org)**

In 2020, you remember that Joe Biden ran for President on the Democratic ticket against Donald Trump for the Republicans. *Do you remember for sure whether or not you voted in that election?*

::: incremental
- Yes, I voted
- No, I did not vote
- I was not eligible to vote
:::

:::aside
[gss.norc.org](https://gss.norc.org)
:::

## GSS Data

```{r setup, include=FALSE}
# Global options for the rendering behavior of all subsequent code chunks
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE,
                      message = FALSE,
                      fig.pos = "center")


```

::: {.panel-tabset}
## Code

```{r, cache = TRUE}
library(tidyverse)
# remotes::install_github("kjhealy/gssr")
library(gssr)

gss22 = gss_get_yr(2022)

# Question: 
# Do you remember for sure whether or not you voted in the last election?
gss = gss22 %>% 
  select(vote20) %>% 
  mutate(vote = ifelse(vote20 == 1, 1, 0)) %>% 
  drop_na()
```


## Data

```{r, echo = FALSE}
gss
```

:::

## Which of the following implies i.i.d?

**Option 1**

```{r, eval = FALSE}
x = sample(gss$vote, size = 10, replace = FALSE)
```

. . .

**Option 2**

```{r, eval = FALSE}
x = sample(gss$vote, size = 10, replace = TRUE)
```

. . .


**Option 3**

```{r, eval = FALSE}
x0 = sample(gss$vote, size = 1)
x1 = sample(gss$vote, size = 1)
x2 = sample(gss$vote, size = 1)
x3 = sample(gss$vote, size = 1)
x4 = sample(gss$vote, size = 1)
x5 = sample(gss$vote, size = 1)
x6 = sample(gss$vote, size = 1)
x7 = sample(gss$vote, size = 1)
x8 = sample(gss$vote, size = 1)
x9 = sample(gss$vote, size = 1)
```

## Pretend our data is the population

**Population mean**

```{r}
mean(gss$vote)
```

. . .

**Sample mean**

```{r}
set.seed(1)
x = sample(gss$vote, size = 10, replace = FALSE)
mean(x)
```

. . .


```{r}
set.seed(2)
x = sample(gss$vote, size = 10, replace = FALSE)
mean(x)
```

. . .

Wait what's the point of random sampling!?

<!-- :::: {.columns} -->

<!-- ::: {.column width="50%"} -->
<!-- **Bias** -->

<!-- ```{r} -->
<!-- mean(x) - mean(gss$vote) -->
<!-- ``` -->

<!-- ::: -->

<!-- ::: {.column width="50%"} -->
<!-- **MSE** -->

<!-- ```{r} -->
<!-- mean((x - mean(gss$vote))^2) -->
<!-- ``` -->

<!-- ::: -->

<!-- :::: -->

## Finite-sample properties

::: {.panel-tabset}
## Code

```{r}
sample_mean = function(n){
  mean(sample(gss$vote, size = n), replace = FALSE)
}

set.seed(20241009)
means = replicate(10, sample_mean(10))
```

## Output

```{r}
means
```

```{r, include = FALSE}
# ggplot global options
library(tidyverse)
theme_set(theme_gray(base_size = 20))
ggdodge = position_dodge(width = 0.5)
```



:::



## Increase the number of replicates

::: {.panel-tabset}
## Function

```{r}
rep_means = function(reps){
  means = replicate(reps, sample_mean(10))
  out = mean(means)
  return(out)
}
```


## Map

```{r}
set.seed(20250115)

obs = seq(10, 500, by = 10) %>% 
  map(~rep_means(.)) %>% 
  unlist()
```

:::



## Result

```{r, echo = FALSE}
weak_law = data.frame(
  reps = seq(10, 500, by = 10),
  avg_prop = obs
)

wl_plot = ggplot(weak_law) +
  aes(x = reps, y = obs) +
  geom_hline(yintercept = 0.74,
             linetype = "dashed") +
  geom_point(color = "#4E2A84", size = 2) +
  labs(x = "Number of attempts",
       y = "Avgerage proportion") +
  scale_x_continuous(breaks = seq(0, 500, by = 100))
```

```{r, cache = TRUE, echo = FALSE}
library(gganimate)

wl_plot +
  transition_manual(reps, cumulative = TRUE)
```

## Result

```{r, echo = FALSE}

ggplot(weak_law) +
  aes(x = reps, y = obs) +
  geom_hline(yintercept = 0.74,
             linetype = "dashed") +
  geom_point(color = "#4E2A84", size = 2) +
  geom_path(color = "#4E2A84") +
  labs(x = "Number of attempts",
       y = "Avgerage proportion") +
  scale_x_continuous(breaks = seq(0, 500, by = 100))
```

## Weak Law of Large Numbers (WLLN)

If you take i.i.d. draws from random variable $X$ a large number of times

. . .

$$
\overline X\xrightarrow{p} E[X]
$$

. . .

Which implies that the sample mean with a large enough sample size has *negligible* probability of being far from the *true* population parameter 

::: aside
It's a **weak** law because the only guarantee is *convergence in probability*. It does not imply that it will ever *reach* the population parameter.
:::

## Asymptotic properties

::: {.panel-tabset}
## Function

```{r}
draw_sample = function(data, n) {
  out = data %>% 
    sample_n(size = n) %>% 
    summarize(
      estimate = mean(vote)
    ) %>% .$estimate
  
  return(out)
}

# Example
draw_sample(gss, 100)
```

## Scale up

```{r}
draw_many = function(reps, data, n){
  temp = replicate(reps, 
                   draw_sample(data, n)) %>% 
    unlist()
  
  out = data.frame(
    n = n,
    prop = temp
  )
  
  return(out)
}


```

## Apply

```{r}
samples = c(10, 100, 500)

set.seed(20250929)
df = samples %>% 
  map(~draw_many(reps = 100, data = gss, n = .)) %>% 
  bind_rows %>% 
  mutate(n = n)
```

## Output

```{r, echo = FALSE}
df
```

:::

## Result

```{r, fig.align='center', echo = FALSE}
df = df %>% 
  mutate(samp = paste(n, "people 100 times"))

ggplot(df) +
  aes(x = prop) + 
  geom_line(
    stat = "density",
    linewidth = 2,
    color = "#4E2A84") +
  labs(y = "Density",
       x = "Proportion who recall voting") +
  xlim(0.4, 1) +
  facet_wrap(~samp)
```

## Central limit theorem (CLT)

As sample size increases, the standardized sample mean

. . .

$$
Z \xrightarrow{d} N(0,1)
$$

. . .

where

$$
Z = \frac{(\overline X - E[\overline X])}{\sigma[\overline X]}
$$


## Central limit theorem (CLT)

As sample size increases, the standardized sample mean


$$
Z \xrightarrow{d} N(0,1)
$$

Or without standardizing

$$
\sqrt{n} (\overline X - E) \xrightarrow{d} N(0, \sigma^2)
$$

. . .

Which implies that if $n$ is large, then the sample mean will **tend** to have a normal distribution 

## Central limit theorem (CLT)

As sample size increases, the standardized sample mean


$$
Z \xrightarrow{d} N(0,1)
$$

Or without standardizing

$$
\sqrt{n} (\overline X - E) \xrightarrow{d} N(0, \sigma^2)
$$


Which implies that if $n$ is large, then the sample mean will **tend** to have a normal distribution *even if the population is not distributed normally*

## Moving on

WLLN and CLT hold for sample means under random sampling

. . .

So you do not need to prove them *empirically* to justify statistical inference

. . .

This is not true for other estimators...

## Example: Variance estimation

[Continue here not sure if show numbers first or formula]

## Variance

Why is there a difference?

. . .

```{r}
set.seed(20241009)
gss %>%
  # replace = FALSE by default
  sample_n(size = 400) %>% 
  summarize(var1 = var(vote),
            var2 = mean(mean(vote^2) - mean(vote)^2),
            var3 = mean(mean(vote^2) - mean(vote)^2) * (400/(400-1))
            )
  
```

## Properties of estimators

::: incremental
- **Unbiasedness:** $E[\widehat \theta] = \theta$

- **Efficiency:**  $\widehat \theta_A$ is more efficient that $\widehat \theta_ B$ if it has lower MSE

- **Consistency:** $\widehat \theta \overset{p}{\to} \theta$
:::

## Next week: Regression (AM Chapter 4) {.smaller}

Important:

- Estimation (Connection to BLP)
- Inference (3 types of standard errors)
- Application (Clean water and infant mortality)

Less important (POLI_SCI 405):

- Polynomials
- Interactions
- Overfitting
- Average partial derivatives vs. partial derivative at the average (aka marginal effects)

Ignore:

- Sieve estimation
- Penalized regression

## Next week: Other readings {.smaller}

**Berk (2010)**

- Regression level I, II, III

**Achen (2005)**

- Why are "garbage-can" regressions a problem?
- Total vs direct effects
- Linearity vs. monotonicity

**Hansen (2022)** (skip proofs)

- What does it mean for OLS to be BUE instead of BLUE? Why does it matter?
- History of OLS