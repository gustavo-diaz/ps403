---
title: "Summarizing Distributions"
subtitle: "POLI_SCI 403: Probability and Statistics"
format: 
  revealjs:
    slide-number: false
    progress: false
    code-overflow: wrap
    chalkboard: true
---

## Agenda

- Revisit random variables
- Summarizing random variables
- Lab

## Last week

In your own words, what is a random variable?

. . .

- *Discrete* random variables have a **PMF**

. . .

- *Continuous* random variables have a **PDF**

. . .

In either case, they are just functions:

$$
f(x) = Pr[X = x]
$$

. . .

They important part is that we can **operate** on them, meaning we can *theorize about producing single number summaries* that describe them

## Expected value

Discrete:

$$
E[X] = \sum_x x f(x)
$$

&nbsp;

Continuous:

$$
E[X] = \int_{-\infty}^{+\infty} xf(x)dx
$$


## Expected value

Discrete:

$$
E[X] = \sum_x x f(x)
$$

&nbsp;

Continuous:

$$
E[X] = \int_{-\infty}^{+\infty} xf(x)dx
$$

::: aside
These are just weighted averages of each value and its corresponding probability
:::



## Linearity of expectations

::: aside
This is a very important property!
:::

Let $X$ and $Y$ be random variables, then $\forall a, b, c \in \mathbb{R}$

. . .

$$
E[aX + bY + c] = a E[X] + bE[Y] + c
$$

## Variance and standard deviation

::: incremental
- **Variance:** $V[X] = E[(X - E[X])^2]$

- **Alternative:** $V[X] = E[X^2] - E[X]^2$

- **Standard deviation:** $\sigma[X] = \sqrt{V[X]}$
:::

## Why do we like the standard deviation?

. . .


$$
V[aX] = a^2 V[X]
$$

. . .

$$
\sigma[aX] = |a| \sigma[X]
$$

## Measuring fit

::: aside
Eventually, $c$ will become something more meaningful
:::

$$
MSE = E[(X-c)^2]
$$

. . .


- $E[X]$ minimizes MSE (cf. Theorems 2.1.23 and 2.1.24)

. . .

$$
E[(X-c)^2] = V[X] + (E[X]-c)^2
$$

## Summarizing joint distributions

. . .

**Covariance**

$$
\text{Cov}[X,Y] = E[(X-E[X])(Y-E[Y])]
$$

. . .

**Correlation:**


$$
\rho[X,Y] = \frac{\text{Cov}[X,Y]}{\sigma[X] \sigma[Y]}
$$

. . .

**Variance rule:** $V[X+Y] = V[X] + \color{purple}{2\text{Cov}[X,Y]} + V[Y]$

## Independence

. . .

What does it mean for $X$ and $Y$ to be **independent**?

. . .

> *Knowing the outcome of one random variable provides no information about the probability of any outcome for the other.*


## Consequences of independence

::: incremental
- $E[XY] = E[X] E[Y]$
- $\text{Cov} [X,Y] = 0$
- $V[X+Y] =V[X] + V[Y]$
:::

. . .

$\rho [X,Y] = 0$ $\not \Rightarrow$ independence

## Rank correlation

. . .

$\rho$ is **Pearson's** correlation

$$
\rho[X,Y] = \frac{\text{Cov}[X,Y]}{\sigma[X] \sigma[Y]}
$$

. . .

**Spearman's**

$$
r_s = \rho[R[X], R[Y]] = \frac{\text{Cov}[R[X],R[Y]}{\sigma[R[X]] \sigma[R[Y]]}
$$

## Rank correlation

**Kendall's**

. . .

$$
\tau = \frac{(\text{# concordant pairs}) - (\text{# discordant pairs})} {(\text{# total pairs})}
$$

. . .

- **concordant:** $x_i > x_j$ *and* $y_i > y_j$ **OR** $x_i < x_j$ *and* $y_i < y_j$

- **discordant** otherwise

## Correlation in R

```{r, eval = FALSE, echo = TRUE}
cor(x, y, method = "pearson") # default

cor(x, y, method = "spearman")

cor(x, y, method = "kendall")
```


## Best (linear) predictor

. . .

CEF $E[Y|X]$ minimizes MSE of $Y$ given $X$

. . .

If we restrict ourselves to a linear functional form $Y = a + bX$, then the following minimize MSE of $Y$ given $X$:

. . .

- $g(X) = \alpha + \beta X$ where
- $\alpha = E[Y] - \frac{\text{Cov}[X,Y]}{V[X]}E[X]$
- $\beta = \frac{\text{Cov}[X,Y]}{V[X]}$


::: aside
For now, just remember this will become very important
:::