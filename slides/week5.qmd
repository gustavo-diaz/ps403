---
title: "Uncertainty"
subtitle: "POLI_SCI 403: Probability and Statistics"
format: 
  revealjs:
    slide-number: false
    progress: false
    code-overflow: wrap
    chalkboard: true
---

# Agenda

```{r setup, include=FALSE}
# Global options for the rendering behavior of all subsequent code chunks
# Set tidy = TRUE if code overflows
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE,
                      message = FALSE,
                      tidy = FALSE) 

# Packages, add more here as needed
library(tidyverse)
library(tinytable)
library(mosaic) # easy resampling

# ggplot global options
theme_set(theme_gray(base_size = 20))

#tinytable math
options(tinytable_html_mathjax = TRUE)
```



- Confidence intervals

- Hypothesis testing

- Lab

# So far

- Random variables to think about statistical properties before collecting data

- i.i.d. sample to enable inference from *estimators* to *estimands*

- Statistical properties of (point) estimators

. . .

**This week:** Convey uncertainty around *estimates*

## Remember

There are two kinds of variance *estimators*

**_Sample_ variance:** $\widehat V[X]$ (...of random variable $X$)

**_Sampling_ variance:** $V[\overline X]$ (...of an estimator)

. . .

We *usually* report **sample** variance (or SD) to describe our data

. . .

We use **sampling** variance to convey uncertainty around the estimates we produce

# Data

## Back to GSS

::: {.panel-tabset}
## Code
```{r}
library(gssr)

gss22 = gss_get_yr(2022)

gss = gss22 %>% 
  select(vote20) %>% 
  mutate(vote = ifelse(vote20 == 1, 1, 0)) %>% 
  drop_na()
```

## Output

```{r}
gss
```
:::


## Pretend whole sample is the population

Make function to get sample mean

```{r}
sample_mean = function(data, n){
  data %>% 
    sample_n(size = n) %>% 
    summarize(
      mean = mean(vote)
    )
}
```

. . .

Check

```{r}
set.seed(1234)
sample_mean(gss, 100)
```

## Repeat many times

::: {.panel-tabset}
## Code

```{r}
set.seed(1234)
vote_df = do(1000) * sample_mean(gss, 100)
```


## Output

```{r}
vote_df
```

:::

## This gives a resampling distribution

::: {.panel-tabset}
## Plot

```{r, echo = FALSE}
#| fig-align: "center"

ggplot(vote_df) +
  aes(x = mean) +
  geom_density(linewidth = 2,
               color = "purple") +
  labs(x = "Estimate",
       y = "Density")
```

## Code

```{r, eval = FALSE}
ggplot(vote_df) +
  aes(x = mean) +
  geom_density(linewidth = 2,
               color = "purple") +
  labs(x = "Estimate",
       y = "Density")
```
:::

## Then we estimate the variance of the resampling distribution

```{r}
vote_df %>% 
  summarize(
    variance = var(mean),
    std.error = sd(mean)
  )
```
# Good?

# Hold on

. . .

No one repeats the study many times!

. . .

If you have resources for 100 participants 1,000 times

. . .

You have resources for 100,000 participants one time!

## What we do instead

Leverage asymptotic properties (CLT) to find a shortcut to calculate uncertainty around our estimate without having to redo the whole study many times

. . .

This is called calculating standard errors (confidence intervals, p-values) via **analytic derivation**

. . .

They are (only) **asymptotically valid** iff *i.i.d.* assumption holds

. . .

Which is implies the CLT will "kick in" with a large enough sample 

# Confidence intervals

## Steps

. . .

Choose $\alpha \in (0,1)$

. . .

**Confidence level** is $100 \times (1-\alpha)$


. . .

Choose estimand $\theta$ and estimator $\widehat \theta$

::: aside
The convention is $\alpha = 0.05$, which gives a 95% confidence interval. There is a good reason for this (coming soon), but nowadays it is mostly path dependency
:::

## Steps

Then we get **normal approximation-based confidence intervals**

$$
CI_{1-\alpha}(\theta) = (\widehat \theta - z_{(1-\frac{\alpha}{2})} \sqrt{\widehat V [\widehat{\theta}]}, \widehat \theta  + z_{(1-\frac{\alpha}{2})} \sqrt{\widehat V [\widehat{\theta}]})
$$ 

. . .

where $z_*$ denotes the quantile of the standard normal distribution $N(0,1)$

::: aside
The convention is $\alpha = 0.05$, which gives a 95% confidence interval. There is a good reason for this (coming soon), but nowadays it is mostly path dependency
:::

## Steps

Then we get **normal approximation-based confidence intervals**

$$
CI_{1-\alpha}(\theta) = (\widehat \theta - z_{(1-\frac{\alpha}{2})} \widehat {SE} [\widehat{\theta}], \widehat \theta  + z_{(1-\frac{\alpha}{2})} \widehat {SE} [\widehat{\theta}])
$$ 

where $z_*$ denotes the quantile of the standard normal distribution $N(0,1)$

::: aside
Clean up a bit to replace with the **standard error** of the estimator
:::

## Why the standard normal?

The idea is that by asymptotic normality

$$
\sqrt{n} (\widehat \theta - \theta) \xrightarrow{d} N(0, \phi^2)
$$

. . .

which is annoying because variance $\phi^2$ is unknown

. . .

But if i.i.d. holds, there is transformation $Z$ with known distribution

. . .

$$
Z \xrightarrow{d} N(0,1)
$$

::: aside
The proof is annoying, it depends on **Slutsky's Theorem** and the **Continuous Mapping Theorem (CMT)**
:::



## Why the standard normal?

```{r, echo = FALSE}

conf = tribble(
  ~`Confidence level`, ~`$\\alpha$`, ~`$z$`,
  "90%", 0.10, 1.64,
  "95%", 0.05, 1.96,
  "99%", 0.01, 2.58
)

conf %>% tt()
```

## Why 95%?


![](fig/normal_conf.webp){fig-align="center" width="80%"}


## Asymptotic validity
