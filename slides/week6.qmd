---
format: 
  revealjs:
    slide-number: false
    progress: false
    chalkboard: true
    code-overflow: wrap
---


```{r setup, include=FALSE}
# Global options for the rendering behavior of all subsequent code chunks
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE,
                      message = FALSE,
                      fig.pos = "center")


```

## What does "parametric" mean?

::: incremental
- **Parametric:** Assume full functional form with finite number of parameters

- **Nonparametric:** Assume functional form is unknown, possibly infinite number of parameters

- **Semiparametric:** ¯\\_(ツ)_/¯
:::

## Ingredients of a parametric model

::: incremental
- Set of functions $g$
- Indexed by parameter vector $\theta$
- Model is true for every $\theta$ if...
- $f_{Y|X}(y|x) = g(y,x; \theta)$
:::

. . .

We assume there exists a function $g$ with parameters $\theta$

. . .

If you know $\theta$, then you can *fully characterize* the PMF/PDF of $Y$ given $X$ ($f_{Y|X}(y|x)$)

## Toy example: Biased coin flip

. . .

$$
f_Y(y) = g(y;p) = 
  \begin{cases}
  1-p &:& y = 0\\
  p &:& y=1\\
  0 &:& \text{otherwise.}
  \end{cases}
$$

::: incremental
- $\theta = p$
- If you know $p$, then you fully know the distribution of random variable $Y$
:::

## Classical linear model

. . .

$$
Y = \boldsymbol{X \beta} + \varepsilon
$$

::: incremental
- $Y$: Response (Outcome)
- $X$: Matrix of predictors (explanatory variables)
- $\boldsymbol{\beta} = (\beta_0, \beta_1, \ldots, \beta_K)^T$ Vector of coefficients
- $\varepsilon \sim N(0, \sigma^2)$ Errors (residuals) i.i.d. normal with expectation zero 
:::

. . .

If you know $\boldsymbol{\beta}$ and $\sigma^2$, then you can fully characterize the PMF/PDF of $Y$ given $X$

## Why do we need $\varepsilon \sim N(0, \sigma^2)$?

::: incremental
- $\boldsymbol{\beta}$ represents the variation in $Y$ that comes from predictors $\boldsymbol{X}$
- $\varepsilon$ represents variation in $Y$ that **cannot** be attributed to $\boldsymbol{X}$
- If errors are i.i.d. then they are also *independent* of $\boldsymbol{X}$
- We need this so that our *estimator* is **unbiased** by definition
:::

. . .

$$
Y = \beta_0 + \beta_1 X_1 + \varepsilon
$$

. . .

$$
Y = \beta_0 + \beta_1 X_1 + \color{purple}{\beta_2 X_2} + \varepsilon
$$

::: aside
[Dr. Huntington-Klein explains it better](https://youtu.be/0Aukw3CdB-Q?si=A6mQhm2l9QE3ObHM)
:::

## Logistic regression

::: aside
Called logit model in the AM textbook
:::

. . .

Parametric model

$$
g(y, \boldsymbol{X}; \beta) = 
\begin{cases}
1-h(\boldsymbol{X\beta}) &:& y = 0\\
h(\boldsymbol{X\beta}) &:& y = 1 \\
0 &:& \text{otherwise.}
\end{cases}
$$

. . .

Let $h(\boldsymbol{X\beta}) = Pr(Y = 1 | X) = p(X)$ for simplicity

## Logistic regression

::: aside
Called logit model in the AM textbook
:::

Parametric model

$$
g(y, \boldsymbol{X}; \beta) = 
\begin{cases}
1-h(\boldsymbol{X\beta}) &:& y = 0\\
h(\boldsymbol{X\beta}) &:& y = 1 \\
0 &:& \text{otherwise.}
\end{cases}
$$

## Logistic regression

::: aside
Called logit model in the AM textbook
:::

Parametric model

$$
g(y, \boldsymbol{X}; \beta) = 
\begin{cases}
1-p(X) &:& y = 0\\
p(X) &:& y = 1 \\
0 &:& \text{otherwise.}
\end{cases}
$$

::: incremental
- $\boldsymbol{\beta} = (\beta_0, \beta_1, \ldots, \beta_K)^T$ 
- $p(X)$ or $h(\boldsymbol{X\beta})$ is the *mean* or *link* function
- We need the link to bound $y$ in the $[0,1]$ range
:::

## Logistic regression


<!-- My old cheatsheet -->
<!-- https://drive.google.com/drive/folders/1y0SwNYAjG5_uDGj7MaaJJoS3TpZ_5HuL -->

For the logit model, the link is the *logistic function*

$$
p(X) = \frac{e^{X\beta}}{1+e^{X\beta}}
$$

. . .


```{r, echo = FALSE, out.width="75%", fig.align='center'}
library(tidyverse)

theme_set(theme_gray(base_size = 25))

ggplot(data = data.frame(x = c(-3,3))) +
  aes(x) +
  stat_function(fun = function(x) exp(x)/(1+exp(x)), n = 100, linewidth = 2) +
  ylim(0,1) +
  labs(y = "p(X)")
  
```

## Logistic regression


For the logit model, the link is the *logistic function*

$$
p(X) = \frac{e^{X\beta}}{1+e^{X\beta}}
$$

. . .

Rearrange to get the *odds ratio*

$$
\frac{p(X)}{1-p(X)} = e^{{X\beta}}
$$

## Logistic regression


Taking the natural logarithm gives the *log odds*

$$
log \left (\frac{p(X)}{1-p(X)} \right) = X\beta
$$

::: incremental
- Weird to interpret, easy to estimate
- It's called *logit* because you need to *log* *it* to make it easier to estimate
- How do we estimate?
:::

## Maximum likelihood estimation (MLE)

<!-- Show intuition with math, then example from ISLR -->



