---
title: "Missing Data"
subtitle: "POLI_SCI 403: Probability and Statistics"
format: 
  revealjs:
    slide-number: false
    progress: false
    code-overflow: wrap
    chalkboard: true
---


```{r setup, include=FALSE}
# Global options for the rendering behavior of all subsequent code chunks
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE,
                      message = FALSE,
                      fig.pos = "center")

library(tidyverse)
library(tinytable)

# tinytable options
options(tinytable_html_mathjax = TRUE)
```

# Agenda

- Identification

- Mising data: Stable outcomes framework

- Approaches to missing outcome data

- Data example (if enough time)

- Multiple imputation (lab)


# Identification

## So far

**Statistical inference:** From *observed* to *unobserved*

. . .

Needs i.i.d. assumption to connect point estimates to population quantities

. . .

**Alternative approach:** We want to tell a story about what we *observe* that is true despite of what remains *unobserved*

. . .

Examples?

. . .

This is the domain of **nonparametric identification**

## Identification

::: incremental
- A "model" is **identifiable** if it is *theoretically* possible to learn its true values

- **Example:** If we can credibly approximate $E(X)$ with the sample mean, then we can say $E(X)$ is *identified*

- **Validity:** An estimator is *valid* if we believe that it measures what it claims to measure

- Validity implies *low bias* and *consistency*

- But remember these properties depend on the data!

:::

## Point-identification

> A statistical functional of a random vector 

## Point-identification

> A statistical functional of a random vector is *point-identified* 

## Point-identification

> A statistical functional of a random vector is *point-identified* by a set of assumptions

## Point-identification

> A statistical functional of a random vector is *point-identified* by a set of assumptions if there exists one and only one value of that statistical functional

## Point-identification

> A statistical functional of a random vector is *point-identified* by a set of assumptions if there exists one and only one value of that statistical functional that is logically compatible with the CDF of the unobservable vector given the assumptions.


# Missing Data

. . .

Should we worry?

## Missing data problem

:::: {.columns}

::: {.column width="50%"}

```{r, echo = FALSE}
tab0 = tribble(
  ~Unit, ~`$Y_i^*$`, ~`$R_i$`, ~`$Y_i$`,
  1, 1, 1, 1,
  2, -99, 0, 0, 
  3, 1, 1, 1,
  4, 0, 1, 0,
  5, 1, 1, 1,
  6, -99, 0, 0
)

tt(tab0) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") %>% 
  style_tt(j = 4,
           color = "gray")
```

:::
::::

## Missing data problem

:::: {.columns}

::: {.column width="50%"}

```{r, echo = FALSE}
tt(tab0) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") %>% 
  style_tt(j = 4,
           color = "gray")
```

:::

::: {.column width="50%"}

**Stable outcomes framework**

::: incremental
- $Y_i$: True outcome (unobserved)

- $R_i$: Recorded (1) or missing (0)

- $Y_i^*$: Observed outcome

- $Y_i^* = Y_i R_i + (-99) (1-R_i)$

- [$E[Y_i] = 3/6$]{style="color:gray;"}; $E[Y_i^*] = 3/4$
:::


:::

::::

. . .

**Challenge:** [$E[Y_i]$]{style="color:gray;"} is **not** *point-identified*

## What can we do?

1. Make minimal assumptions

2. Make heroic assumptions


## Minimal assumption: $Y_i$ is *bounded*


```{r, echo = FALSE}
tab1 = tribble(
  ~Unit, ~`$Y_i^*$`, ~`$R_i$`,
  1, 1, 1,
  2, -99, 0,
  3, 1, 1,
  4, 0, 1,
  5, 1, 1,
  6, -99, 0
)

tt(tab1) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") 
```

. . .

Plug-in worst case scenario to find *lower* and *upper* bound

## Minimal assumption: $Y_i$ is *bounded*


```{r, echo = FALSE}
tab2 = tribble(
  ~Unit, ~`$Y_i^*$`, ~`$R_i$`, ~`$Y_i^L$`, ~`$Y_i^U$`,
  1, 1, 1, 1, 1, 
  2, -99, 0, 0, 1,
  3, 1, 1, 1, 1,
  4, 0, 1, 0, 0,
  5, 1, 1, 1, 1,
  6, -99, 0, 0, 1
)

tt(tab2) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") %>% 
  style_tt(
    i = 1:6,
    j = 4:5,
    color = "white"
  )
```

Plug-in worst case scenario to find *lower* and *upper* bound

## Minimal assumption: $Y_i$ is *bounded*


```{r, echo = FALSE}
tt(tab2) %>% 
  style_tt(i = c(2, 6),
           j = c(2, 4),
           color = "purple") %>% 
  style_tt(
    i = 1:6,
    j = 5,
    color = "white"
  )
```

Plug-in worst case scenario to find *lower* and *upper* bound

## Minimal assumption: $Y_i$ is *bounded*


```{r, echo = FALSE}
tt(tab2) %>% 
  style_tt(i = c(2, 6),
           j = c(2, 4, 5),
           color = "purple")
```

Plug-in worst case scenario to find *lower* and *upper* bound

## Minimal assumption: $Y_i$ is *bounded*


```{r, echo = FALSE}
tt(tab2) %>% 
  style_tt(i = c(2, 6),
           j = c(2, 4, 5),
           color = "purple")
```

**Sharp bounds:** $\widehat E[Y_i] = [\frac{3}{6}, \frac{5}{6}]$

## More formally

You can write plug-in estimator as a *weighted average*

. . .

$$
\textbf{Lower bound: } \frac{1}{n}\sum_{i=1}^n (Y_i^*R_i + \mathbf{a} (1-R_i))
$$

. . .

$$
\textbf{Upper bound: } \frac{1}{n}\sum_{i=1}^n (Y_i^*R_i + \mathbf{b} (1-R_i))
$$

. . .

Meaning you do not need to know which are missing observations, just how many!

::: aside
$\mathbf a$ and $\mathbf b$ are the lowest/highest possible value of $Y_i$, respectively
:::


## Why *sharp* bounds?

. . .

They are **sharp** because we cannot improve them without additional assumptions that do not follow from the data

. . .

So we say $\widehat E[Y_i]$ is *partially identified* under these assumptions

. . .

Size of bounds is proportional to amount of missing data

. . .

And sometimes you can use *auxiliary information* to refine bounds

. . .

**Challenge:** Social scientists don't know what to do with bounds

::: aside
IDK exactly why "minimal assumptions" means "sharp", but this is a related yet different use of "sharp" compared to the concept of "sharp null" next week
:::


## More heroic assumptions

. . .

**Baseline:** Missing not at random (MNAR/NMAR) 

$$Y_i \not\!\perp\!\!\!\perp R_i$$

. . .

**MCAR:** Missing completely at random

$$
Y_i \perp \!\!\! \perp R_i
$$

. . .

**MAR:** Missing at random *after conditioning on covariates* (aka *ignorability*)

$$
Y_i \perp \!\!\! \perp R_i | \mathbf{X}_i
$$

## MCAR

```{r, echo = FALSE}

tt(tab1) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") 
```

. . .

We assume the *distribution of outcomes is the same* for observed and unobserved

## MCAR

```{r, echo = FALSE}

tt(tab1) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") 
```

Which is equivalent to imputing missing obs with the sample mean


## MCAR

```{r, echo = FALSE}
tab3 = tribble(
  ~Unit, ~`$Y_i^*$`, ~`$R_i$`, ~`$Y_i$`,
  1, 1, 1, "1",
  2, -99, 0, "$\\widehat E[Y_i] = 3/4$",
  3, 1, 1, "1",
  4, 0, 1, "0",
  5, 1, 1, "1",
  6, -99, 0, "$\\widehat E[Y_i] = 3/4$"
)


tt(tab3) %>% 
  style_tt(i = c(2, 6),
           j = c(2, 4),
           color = "purple")
```

Which is equivalent to imputing missing obs with the sample mean

## MAR

```{r, echo = FALSE}

tt(tab1) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") 
```

. . .

We assume *conditional independence*

## MAR

```{r, echo = FALSE}

tt(tab1) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") 
```

Imagine we now observe covariate $X_i$


## MAR

```{r, echo = FALSE}
tab4 = tribble(
  ~Unit, ~`$Y_i^*$`, ~`$R_i$`, ~`$X_i$`,
  1, 1, 1, 0,
  2, -99, 0, 0,
  3, 1, 1, 0,
  4, 0, 1, 0,
  5, 1, 1, 1,
  6, -99, 0, 1
)

tt(tab4) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") 
```

Imagine we now observe covariate $X_i$

## MAR

```{r, echo = FALSE}
tt(tab4) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") 
```

MAR lets us impute the sample mean of units with the same covariate values

## MAR

```{r, echo = FALSE}
tab5 = tribble(
  ~Unit, ~`$Y_i^*$`, ~`$R_i$`, ~`$X_i$`, ~`$Y_i$`,
  1, 1, 1, 0, "1",
  2, -99, 0, 0, "$\\widehat E[Y_i|X_i=0] = 2/3$",
  3, 1, 1, 0, "1",
  4, 0, 1, 0, "0",
  5, 1, 1, 1, "1",
  6, -99, 0, 1, "$\\widehat E[Y_i|X_i=1] = 1$"
)

tt(tab5) %>% 
  style_tt(i = c(2, 6),
           j = c(2, 5),
           color = "purple") 
```

We can impute the sample mean of units with the same covariate values

## MAR with multiple covariates

```{r, echo = FALSE}

tt(tab1) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") 
```

Imagine we observe covariates $X_{1i}$ and $X_{2i}$

## MAR with multiple covariates

```{r, echo = FALSE}
tab6 = tribble(
  ~Unit, ~`$Y_i^*$`, ~`$R_i$`, ~`$X_{1i}$`, ~`$X_{2i}$`,
  1, 1, 1, 0, 3,
  2, -99, 0, 0, 7,
  3, 1, 1, 0, 9,
  4, 0, 1, 0, 5,
  5, 1, 1, 1, 4,
  6, -99, 0, 1, 3
)

tt(tab6) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") 
```

Imagine we observe covariates $X_{1i}$ and $X_{2i}$

## Options

1. Regression estimation

2. Hot deck imputation

3. Inverse-probability weighting (IPW)

## Regression estimation

```{r, echo = FALSE}
tt(tab6) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") 
```


. . .

Estimate $\widehat Y_i^* = \widehat \beta_0 + \widehat \beta_1 X_{1i} + \widehat \beta_2 X_{2i}$ with observed data

## Regression estimation

```{r, echo = FALSE}
tt(tab6) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") 
```

Use coefficients to impute missing values

## Regression estimation

```{r, echo = FALSE}
tab7 = tribble(
  ~Unit, ~`$Y_i^*$`, ~`$R_i$`, ~`$X_{1i}$`, ~`$X_{2i}$`, ~`$Y_i$`,
  1, 1, 1, 0, 3, "1",
  2, -99, 0, 0, 7, "$\\widehat \\beta_0 + \\widehat \\beta_1 \\cdot 0 + \\widehat \\beta_2 \\cdot 7$",
  3, 1, 1, 0, 9, "1",
  4, 0, 1, 0, 5, "0",
  5, 1, 1, 1, 4, "1",
  6, -99, 0, 1, 3, "$\\widehat \\beta_0 + \\widehat \\beta_1 \\cdot 1 + \\widehat \\beta_2 \\cdot 3$"
)

tt(tab7) %>% 
  style_tt(i = c(2, 6),
           j = c(2, 6),
           color = "purple") 
```

. . .

Which yields the same $\widehat E[Y_i]$ as

## Regression estimation

```{r, echo = FALSE}
tab8 = tribble(
  ~Unit, ~`$Y_i^*$`, ~`$R_i$`, ~`$X_{1i}$`, ~`$X_{2i}$`, ~`$Y_i$`,
  1, 1, 1, 0, 3, "$\\widehat \\beta_0 + \\widehat \\beta_1 \\cdot 0 + \\widehat \\beta_2 \\cdot 3$",
  2, -99, 0, 0, 7, "$\\widehat \\beta_0 + \\widehat \\beta_1 \\cdot 0 + \\widehat \\beta_2 \\cdot 7$",
  3, 1, 1, 0, 9, "$\\widehat \\beta_0 + \\widehat \\beta_1 \\cdot 0 + \\widehat \\beta_2 \\cdot 9$",
  4, 0, 1, 0, 5, "$\\widehat \\beta_0 + \\widehat \\beta_1 \\cdot 0 + \\widehat \\beta_2 \\cdot 5$",
  5, 1, 1, 1, 4, "$\\widehat \\beta_0 + \\widehat \\beta_1 \\cdot 1 + \\widehat \\beta_2 \\cdot 4$",
  6, -99, 0, 1, 3, "$\\widehat \\beta_0 + \\widehat \\beta_1 \\cdot 1 + \\widehat \\beta_2 \\cdot 3$"
)

tt(tab8) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") %>% 
  style_tt(i = 1:6,
           j = 6,
           color = "purple")
```

. . .

What if we could condition on a *function* of covariates instead?

## Propensity score

. . .

**Response propensity function**

$$
p_R(\mathbf{x}) = Pr[R_i = 1 | \mathbf{X}_i = x]
$$

::: incremental
- $p_R(\mathbf{X}_i)$ is the **propensity score** of unit $i$

- Conditional probability of response given the covariates

- Useful since you cannot condition on many covariates  *nonparametrically* 

- Can estimate *consistently* via ML (usually logit)

- Still need to assume *specification* is correct
:::

## Hot deck imputation

```{r, echo = FALSE}
pstab = tribble(
  ~Unit, ~`$Y_i^*$`, ~`$R_i$`, ~`$X_{1i}$`, ~`$X_{2i}$`,
  1, 2, 1, 0, 3,
  2, -99, 0, 0, 7,
  3, 3, 1, 0, 9,
  4, 10, 1, 0, 5,
  5, 12, 1, 1, 4,
  6, -99, 0, 1, 3
) 

tt(pstab) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple")
  
```

. . .

Add propensity score

## Hot deck imputation

```{r, echo = FALSE}
pstab$`$p_R(\\mathbf{X}_i)$` = c(0.33, 0.14, 0.73, 0.35, 0.78, 0.70)

tt(pstab) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple")
```

Add propensity score

## Hot deck imputation

```{r, echo = FALSE}


tt(pstab) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple")
```

Impute values with nearest propensity score

## Hot deck imputation

```{r, echo = FALSE}
pstab$`$Y_i$` = c(
  "2",
  "$\\widehat E[Y_i|p_R(X_i) = 0.14]$",
  "3",
  "10",
  "12",
  "$\\widehat E[Y_i|p_R(X_i) = 0.70]$"
)

tt(pstab) %>% 
  style_tt(i = c(2, 6),
           j = c(2,7),
           color = "purple")
```

Impute values with nearest propensity score

## Hot deck imputation

```{r, echo = FALSE}
pstab$`$Y_i$` = c(2, 2, 3, 10, 12, 3)

tt(pstab) %>% 
  style_tt(i = c(2, 6),
           j = c(2,7),
           color = "purple")
```

Impute values with nearest propensity score

## Hot deck what?

![](https://www.hnf.de/uploads/tx_templavoila/Kartenlocher_HNF_1.JPG){fig-align="center"}



## Hot deck imputation

```{r, echo = FALSE}
pstab$`$Y_i$` = c(2, 2, 3, 10, 12, 3)

tt(pstab) %>% 
  style_tt(i = c(2, 6),
           j = c(2,7),
           color = "purple")
```

Impute values with nearest propensity score

## Hot deck imputation

```{r, echo = FALSE}
tt(pstab) %>% 
  style_tt(i = c(2, 6),
           j = c(2,7),
           color = "purple")
```

**Not a good idea.** Discards a lot of information!

## Inverse probability weighting (IPW)

::: incremental
- The problem is not missing data

- But an unrepresentative sample

- If MAR holds, we can use propensity score to *weight* observations

- **Inverse probability weight** $\frac{1}{p_R(\mathbf{X}_i)}$

:::

## IPW estimator


**Horvitz-Thompson estimator**

$$
\widehat E_{IPW}[Y_i] = \frac{1}{n} \sum_{i=1}^n \frac{Y_i^* R_i}{\widehat p_R(\mathbf{X}_i)}
$$

::: incremental

- Low propensity scores are *underrepresented* $\rightarrow$ Assign greater weight

- High-propensity scores $\rightarrow$ lower weight

:::

::: aside
In practice, HÃ¡jek's stabilized IPW estimator is used more often because it avoids disproportional weights (cf. Definition 6.2.8 in AM Chapter 6)
:::

## Recap

. . .

**Options under MAR**

1. Regression estimation

2. Hot deck imputation

3. Inverse-probability weighting (IPW)

## Recap

**Options under MAR**

1. Regression estimation

2. ~~Hot deck imputation~~

3. Inverse-probability weighting (IPW)

## Recap

**Options under MAR**

1. Regression estimation

2. ~~Hot deck imputation~~

3. Inverse-probability weighting (IPW)

4. **Doubly robust estimation**

## Doubly robust estimation

::: incremental
- Regression specification may be incorrect

- Propensity score specification may be incorrect

- Combining the two in a model that yields consistent estimates when *either* is correct
:::

. . .

$$
\begin{align}
& \widehat E_{DR}[Y_i] = \\
&\frac{1}{n} \sum_{i=1}^n \widehat E[Y_i^*|R_i = 1, \mathbf{X}_i] + \frac{1}{n} \sum_{i=1}^n \frac{R_i(Y_i^* - \widehat E[Y_i^* | R = 1, \mathbf{X}_i])}{\widehat{p}_R(\mathbf{X}_i)}
\end{align}
$$

## Doubly robust estimation

- Regression specification may be incorrect

- Propensity score specification may be incorrect

- Combining the two in a model that yields consistent estimates when *either* is correct


$$
\widehat E_{DR}[Y_i] = \text{Regression estimator} + \text{IPW correction}
$$

## With data


```{r}
library(gssr)

gss22 = gss_get_yr(2022)

gss = gss22 %>% 
  select(vote20) %>% 
  mutate(vote = ifelse(vote20 == 1, 1, 0))

gss %>% 
  group_by(vote) %>% 
  tally()
```


## Bounds as weighted average

Get proportion missing

```{r}
pr_response = mean(!is.na(gss$vote))

pr_response
```

. . .

Lower bound:

```{r}
mean(gss$vote, na.rm = TRUE) * pr_response + 0 *(1-pr_response)
```

. . .

Upper bound:

```{r}
mean(gss$vote, na.rm = TRUE) * pr_response + 1 * (1-pr_response)
```

## Bounds by imputation

```{r}
gss %>% 
  select(vote) %>% 
  mutate(
    vote_lo = ifelse(is.na(vote), 0, vote),
    vote_hi = ifelse(is.na(vote), 1, vote)) %>% 
  summarize(
    complete_cases = mean(vote, na.rm = TRUE),
    lower_bound = mean(vote_lo),
    upper_bound = mean(vote_hi)
  )
```

## MCAR by imputation

```{r}
vote_mean = mean(gss$vote, na.rm = TRUE)

gss %>% 
  select(vote) %>% 
  mutate(
    vote_mcar = ifelse(is.na(vote), vote_mean, vote)) %>% 
  summarize(
    complete_cases = mean(vote, na.rm = TRUE),
    mcar = mean(vote_mcar)
  )
```

::: aside
The number is the same so the real value would come from being able to use more data in general. But also you should never do mean imputation.
:::

## Regression imputation (MAR)

Make a new data with two more variables

```{r}
gss_wide = gss22 %>% 
  select(vote20, sex, partyid) %>% 
  mutate(vote = ifelse(vote20 == 1, 1, 0),
         female = ifelse(sex == 2, 1, 0),
         dem = ifelse(partyid <= 2, 1, 0)) %>% 
  select(vote, female, dem)

head(gss_wide, 3)
```

::: aside
In practice, you want to convince yourself that you have enough variables to justify *conditional independence*
:::

## Regression imputation (MAR)

```{r}
outcome_model = lm(vote ~ female + dem,
                   data = gss_wide)

gss_wide$vote_pred = predict(outcome_model, 
                    # newdata is by default current data in predict.lm
                    newdata = gss_wide)

head(gss_wide)
```

## Regression imputation (MAR)

```{r}
gss_wide %>% 
  mutate(
    vote_reg = ifelse(is.na(vote), vote_pred, vote)
  ) %>% 
  summarize(
    complete_cases = mean(vote, na.rm = TRUE),
    # Some still missing have missing data in the covariates
    mar_reg_1 = mean(vote_reg, na.rm = TRUE),
    mar_reg_2 = mean(vote_pred, na.rm = TRUE)
  )
```


## IPW (MAR)

```{r}
# Make a binary indicator of missingness
gss_wide = gss_wide %>% 
  mutate(
    response = ifelse(!is.na(vote), 1, 0)
  ) %>% 
  filter(!is.na(female) & !is.na(dem))

pscore_model = glm(
  response ~ female + dem,
  family = binomial,
  data = gss_wide
)

# Create variable with probabilities
gss_wide$pscore = predict(pscore_model, type = "response")

# Convert to IPW
gss_wide$ipw = 1/gss_wide$pscore
```

## IPW (MAR)

```{r}
gss_wide %>% 
  summarize(
    complete_cases = mean(vote, na.rm = TRUE),
    mar_ipw = weighted.mean(vote, w = ipw, na.rm = TRUE)
    )
```


## Doubly robust (MAR)

```{r}
doubly_robust = lm(vote ~ female + dem,
                   data = gss_wide,
                   weights = ipw)

gss_wide$vote_dr = predict(doubly_robust,
                           newdata = gss_wide)

gss_wide %>% 
  summarize(
    complete_cases = mean(vote, na.rm = TRUE),
    mar_dr = mean(vote_dr)
  )
```

## Wrapping up

- Missing data means i.i.d. assumption does not hold even with a random/representative sample

- But i.i.d. is assumed out of convenience, not accuracy

- Many are unbothered by missing data because it does not make a difference

- But sometimes ignoring may present problems for inference or the application of other methods

- You want to (at least) show that ignoring missing data is not a big deal
