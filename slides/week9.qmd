---
format: 
  revealjs:
    slide-number: false
    progress: false
    chalkboard: true
    code-overflow: wrap
    code-line-numbers: false
---


```{r setup, include=FALSE}
# Global options for the rendering behavior of all subsequent code chunks
knitr::opts_chunk$set(echo = FALSE, 
                      warning = FALSE,
                      message = FALSE,
                      fig.pos = "center")

library(tidyverse)
library(tinytable)
library(kableExtra) # too lazy to change
library(DeclareDesign) # Re-randomize
library(infer)

# tinytable options
# options(tinytable_html_mathjax = TRUE)
```


## Strategy 2: Ignorability

. . .

What if we cannot assume random assignment?

## Strategy 2: Ignorability

We can still conduct valid causal inference if we believe treatment assignment is **strongly ignorable** (conditional on covariates)

. . .

Requirements:

::: incremental
- *Conditional independence*: $(Y_i(0), Y_i(1)) \perp \!\!\! \perp D_i | \mathbf{X}_i$
- *Positivity*: There exists $\epsilon > 0$ such that $$\epsilon < Pr[D_i = 1 | \mathbf{X}_i] < 1 - \epsilon$$
:::

::: aside
*Weak ignorability* only requires $Y_i(d) \perp \!\!\! \perp D_i | \mathbf{X}_i \forall d \in \{0, 1\}$, which in most cases makes no difference
:::

## Strategy 2: Ignorability

If strong ignorability holds:

::: incremental
- ATE $E[\tau_i]$  is **point-identified**
- Conditional ATE (CATE) $E[\tau_i | \mathbf{X}_i]$ is **point-identified**
:::

. . .

Estimation strategies:

::: incremental
- Conditioning on covariates (regression)
- Propensity score matching
- IPW estimation
- Doubly robust estimation
:::

## Strategy 3 {.center}


What if we could guarantee strong ignorability *by design*?

## Quasi-experiments

::: incremental
- *Observational* studies
- Data structured such that we can credibly make causal claims
- We can *treat* them as experiments but they **are not**
:::

. . .

Examples:

1. Regression discontinuity design (RDD)

2. Difference-in-differences (DID)

## RDD ingredients

::: incremental
- $Y_i$: outcome

- $X_i$: score or running variable

- $c$: cutoff or threshold

- $T_i$: Treatment indicator
:::

. . .

**Potential outcomes**

$$
Y_i = (1 - T_i) Y_i(0) + T_i Y_i(1) = \begin{cases}
Y_i(0) & \text{if } X_i < c\\
Y_i(1) & \text{if } X_i \geq c
\end{cases}
$$

## Sharp RDD

![](fig/rd_assign.png){fig-align="center" width=59%}

::: aside
Called *sharp* because treatment assignment is *deterministic* at the cutoff
:::

## Interpretation: Two approaches

:::: {.columns}

::: {.column width="60%"}

![](fig/rd_assign.png)

:::

::: {.column width="40%"}


1. Local randomization

2. Continuity-based


:::
::::

## Local randomization approach

:::: {.columns}
::: {.column width="60%"}
![](fig/rd_rand.png)
:::

::: {.column width="40%"}
::: incremental
- Bandwidth $\mathcal{W} = [câˆ’w,c+w]$

- Treatment **as-if** random within $\mathcal{W}$

- ATE *point-identified* within $\mathcal{W}$

:::

:::

::::


## Local randomization approach

:::: {.columns}
::: {.column width="60%"}
![](fig/rd_rand.png)
:::

::: {.column width="40%"}

**Requirements**

::: incremental
1. Known probability distribution of scores within $\mathcal{W}$ ($\equiv$ random assignment)

2. Potential outcomes **not affected by scores** within $\mathcal{W}$

:::

:::

::::

## Local randomization approach

:::: {.columns}
::: {.column width="60%"}
![](fig/rd_rand.png)
:::

::: {.column width="40%"}

**Estimation**

::: incremental
- Difference in means within $\mathcal{W}$
:::

**Inference**

::: incremental

1. Randomization inference

2. Normal-approximation (*super-population*)

:::

:::

::::

## Continuity-based approach

:::: {.columns}
::: {.column width="60%"}
![](fig/rd_cont.png)
:::

::: {.column width="40%"}
::: incremental
- ATE is *point-identified* at cutoff

- $\tau_{SRD} \equiv E[Y_i(1) - Y_i(0) | X_i = c]$

- But it does not exist!

- Still, we can approximate

:::

:::

::::

## Continuity-based approach

:::: {.columns}
::: {.column width="60%"}
![](fig/rd_cont.png)
:::

::: {.column width="40%"}

- ATE is *point-identified* at cutoff

- $\tau_{SRD} \equiv E[Y_i(1) - Y_i(0) | X_i = c]$

- But it does not exist!

- Still, we can approximate

$$
\lim_{x \downarrow c} E[Y_i | X_i = x] -
\lim_{x \uparrow c} E[Y_i | X_i x]
$$
:::

::::

## Local polynomial point estimation

**Steps**

::: incremental
1. Choose polynomial $p$ and kernel function $K(\cdot)$

2. Choose bandwidth $h$

3. Fit $\widehat \mu_+$ and $\widehat \mu_-$ via *weighted least-squares* (based on $K(\cdot)$)

4. Estimate: $\widehat \tau_{SRD} = \widehat \mu_+ - \widehat \mu_-$

5. Inference: Correct for adaptive bandwidth selection
:::

::: aside
This a non-parametric procedure since most choices are automated to find optimal MSE
:::

## Polynomial $p$

::: aside
Different polynomials yield different estimates
:::

![](fig/rdd_poly.png){fig-align="center" width=90%}

## Kernel function $K(\cdot)$

![](fig/rd_kernel.png){fig-align="center" width=60%}

::: aside
$K(\cdot)$ assigns weights to units based on distance to cutoff (triangular performs better)
:::

## Bandwidth $h$

::: aside
**Narrower:** less bias, more variance. 
**Wider:** more bias, less variance 
:::

![](fig/rdd_bw2.png){fig-align="center" width=90%}

## DID ingredients

::: incremental

- Time periods: $t = \{1,2\}$ (Before/after treatment)

- Treatment: $D_i = \{0,1\}$

- Potential outcomes: $Y_{i,t}(0) = Y_{i,t}(0, 0)$ and $Y_{i,t}(1) = Y_{i,t}(0, 1)$

:::

. . .

**Switching equation**

$$
Y_{i,t} = D_i Y_{i,t}(1) + (1-D_i) Y_{i,t}(0)
$$


::: aside
Same as canonical potential outcomes framework but now with time periods
:::

## Causal estimand

. . .

**Average treated effect on the treated (ATT) in $t=2$**

$$
\tau_2 = E[Y_{i,2}(1) - Y_{i,2}(0) | D_i = 1]
$$

::: incremental
- Cannot observe directly

- Cannot avoid selection bias

- But before/after setup allows for valid estimation
:::

## Causal estimand

![](fig/dd.png)

## DID estimation

![](fig/dd.png)


$$
\widehat{ATT} =  [\text{Mean}(B) - \text{Mean}(A)] - [\text{Mean}(D) - \text{Mean}(C)]
$$



## DID estimation



![](fig/dd.png)

$$
\widehat{ATT} =  \underbrace{[\text{Mean}(B) - \text{Mean}(A)]}_\text{Difference} - \underbrace{[\text{Mean}(D) - \text{Mean}(C)]}_\text{Difference}
$$

## DID estimation

![](fig/dd.png)

$$
\widehat{ATT} =  \underbrace{\underbrace{[\text{Mean}(B) - \text{Mean}(A)]}_\text{Difference} - \underbrace{[\text{Mean}(D) - \text{Mean}(C)]}_\text{Difference}}_\text{Difference-in-differences}
$$

## Rewrite as a regression estimator

. . .

**Two time periods**


$$
\begin{align}
Y = & \beta_0 + \beta_1 \text{Treated} + \beta_2 \text{Post-treatment} + \\
& \color{purple}{\beta_3 \text{Treated} \times \text{Post-treatment}} + \varepsilon
\end{align}
$$

. . .


```{r, echo=TRUE, eval=FALSE}
# Both are the same
lm(Y ~ Treated * Post-treatment, data = dat)

lm(Y ~ Treated + Post-treatment + Treated:Post-treatment, data= dat)
```


## Rewrite as a regression estimator

**Multiple time periods**

. . .

$$
Y = \alpha_i + \alpha_t + \color{purple}{\beta_1 \text{Treated}} + \varepsilon
$$

. . .

Known as the two-way fixed-effects estimator (TWFE)

. . .

```{r, echo = TRUE, eval = FALSE}
lm_robust(
  Y ~ Treated,
  fixed_effects = ~Unit + Time,
  clusters = Unit,
  se_type = "stata", # faster computing
  data = dat
)
```

## Assumption: Parallel trends

![](fig/dd_trend_1.png)

## What if we break parallel trends?

![](fig/dd_trend_2.png)

## What if we break parallel trends?

![](fig/dd_trend_3.png)

## Challenge: Staggered adoption


![](fig/dd_timing.png)

::: aside
See [Roth et al 2023](https://doi.org/10.1016/j.jeconom.2023.03.008) and [`did`](https://bcallaway11.github.io/did/) package for details
:::

