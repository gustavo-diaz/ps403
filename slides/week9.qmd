---
title: "Missing Data"
subtitle: "POLI_SCI 403: Probability and Statistics"
format: 
  revealjs:
    slide-number: false
    progress: false
    code-overflow: wrap
    chalkboard: true
---


```{r setup, include=FALSE}
# Global options for the rendering behavior of all subsequent code chunks
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE,
                      message = FALSE,
                      fig.pos = "center")

library(tidyverse)
library(tinytable)

# tinytable options
options(tinytable_html_mathjax = TRUE)
```

# Agenda

- Identification

- Stable outcomes framework

- Approaches to missing outcome data

- Multiple imputation (lab)


# Identification

## So far

**Statistical inference:** From *observed* to *unobserved*

. . .

Needs i.i.d. assumption to connect point estimates to population quantities

. . .

**Alternative approach:** We want to tell a story about what we *observe* that is true despite of what remains *unobserved*

. . .

Examples?

. . .

This is the domain of **nonparametric identification**

## Identification

::: incremental
- A "model" is **identifiable** if it is *theoretically* possible to learn its true values

- **Example:** If we can credibly approximate $E(X)$ with the sample mean, then we can say $E(X)$ is *identified*

- **Validity:** An estimator is *valid* if we believe that it measures what it claims to measure

- Validity implies *low bias* and *consistency*

- But remember these properties depend on the data!

:::

## Point-identification

> A statistical functional of a random vector 

## Point-identification

> A statistical functional of a random vector is *point-identified* 

## Point-identification

> A statistical functional of a random vector is *point-identified* by a set of assumptions

## Point-identification

> A statistical functional of a random vector is *point-identified* by a set of assumptions if there exists one and only one value of that statistical functional

## Point-identification

> A statistical functional of a random vector is *point-identified* by a set of assumptions if there exists one and only one value of that statistical functional that is logically compatible with the CDF of the unobservable vector given the assumptions.

## Missing data problem

:::: {.columns}

::: {.column width="50%"}

```{r, echo = FALSE}
tab0 = tribble(
  ~Unit, ~`$Y_i^*$`, ~`$R_i$`, ~`$Y_i$`,
  1, 1, 1, 1,
  2, -99, 0, 0, 
  3, 1, 1, 1,
  4, 0, 1, 0,
  5, 1, 1, 1,
  6, -99, 0, 0
)

tt(tab0) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") %>% 
  style_tt(j = 4,
           color = "gray")
```

:::
::::

## Missing data problem

:::: {.columns}

::: {.column width="50%"}

```{r, echo = FALSE}
tt(tab0) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") %>% 
  style_tt(j = 4,
           color = "gray")
```

:::

::: {.column width="50%"}

**Stable outcomes framework**

::: incremental
- $Y_i$: True outcome (unobserved)

- $R_i$: Recorded (1) or missing (0)

- $Y_i^*$: Observed outcome

- $Y_i^* = Y_i R_i + (-99) (1-R_i)$

- [$E[Y_i] = 3/6$]{style="color:gray;"}; $E[Y_i^*] = 3/4$
:::


:::

::::

. . .

**Challenge:** [$E[Y_i]$]{style="color:gray;"} is **not** *point-identified*

## What can we do?

1. Make minimal assumptions

2. Make heroic assumptions


## Minimal assumption: $Y_i$ is *bounded*


```{r, echo = FALSE}
tab1 = tribble(
  ~Unit, ~`$Y_i^*$`, ~`$R_i$`,
  1, 1, 1,
  2, -99, 0,
  3, 1, 1,
  4, 0, 1,
  5, 1, 1,
  6, -99, 0
)

tt(tab1) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") 
```

. . .

Plug-in worst case scenario to find *lower* and *upper* bound

## Minimal assumption: $Y_i$ is *bounded*


```{r, echo = FALSE}
tab2 = tribble(
  ~Unit, ~`$Y_i^*$`, ~`$R_i$`, ~`$Y_i^L$`, ~`$Y_i^U$`,
  1, 1, 1, 1, 1, 
  2, -99, 0, 0, 1,
  3, 1, 1, 1, 1,
  4, 0, 1, 0, 0,
  5, 1, 1, 1, 1,
  6, -99, 0, 0, 1
)

tt(tab2) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") %>% 
  style_tt(
    i = 1:6,
    j = 4:5,
    color = "white"
  )
```

Plug-in worst case scenario to find *lower* and *upper* bound

## Minimal assumption: $Y_i$ is *bounded*


```{r, echo = FALSE}
tt(tab2) %>% 
  style_tt(i = c(2, 6),
           j = c(2, 4),
           color = "purple") %>% 
  style_tt(
    i = 1:6,
    j = 5,
    color = "white"
  )
```

Plug-in worst case scenario to find *lower* and *upper* bound

## Minimal assumption: $Y_i$ is *bounded*


```{r, echo = FALSE}
tt(tab2) %>% 
  style_tt(i = c(2, 6),
           j = c(2, 4, 5),
           color = "purple")
```

Plug-in worst case scenario to find *lower* and *upper* bound

## Minimal assumption: $Y_i$ is *bounded*


```{r, echo = FALSE}
tt(tab2) %>% 
  style_tt(i = c(2, 6),
           j = c(2, 4, 5),
           color = "purple")
```

**Sharp bounds:** $\widehat E[Y_i] = [\frac{3}{6}, \frac{5}{6}]$

## More formally

You can write plug-in estimator as a *weighted average*

. . .

$$
\textbf{Lower bound: } \frac{1}{n}\sum_{i=1}^n (Y_i^*R_i + \mathbf{a} (1-R_i))
$$

. . .

$$
\textbf{Upper bound: } \frac{1}{n}\sum_{i=1}^n (Y_i^*R_i + \mathbf{b} (1-R_i))
$$

. . .

Meaning you do not need to know which are missing observations, just how many!

::: aside
$\mathbf a$ and $\mathbf b$ are the lowest/highest possible value of $Y_i$, respectively
:::


## Why *sharp* bounds?

. . .

They are **sharp** because we cannot improve them without additional assumptions that do not follow from the data

. . .

So we say $\widehat E[Y_i]$ is *partially identified* under these assumptions

. . .

Size of bounds is proportional to amount of missing data

. . .

And sometimes you can use *auxiliary information* to refine bounds

. . .

**Challenge:** Social scientists don't know what to do with bounds

::: aside
IDK exactly why "minimal assumptions" means "sharp", but this is a related yet different use of "sharp" compared to the concept of "sharp null" next week
:::


## More heroic assumptions

. . .

**Baseline:** Missing not at random (MNAR/NMAR) 

$$Y_i \not\!\perp\!\!\!\perp R_i$$

. . .

**MCAR:** Missing completely at random

$$
Y_i \perp \!\!\! \perp R_i
$$

. . .

**MAR:** Missing at random *after conditioning on covariates* (aka *ignorability*)

$$
Y_i \perp \!\!\! \perp R_i | \mathbf{X}_i
$$

## MCAR

```{r, echo = FALSE}

tt(tab1) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") 
```

. . .

We assume the *distribution of outcomes is the same* for observed and unobserved

## MCAR

```{r, echo = FALSE}

tt(tab1) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") 
```

Which is equivalent to imputing missing obs with the sample mean


## MCAR

```{r, echo = FALSE}
tab3 = tribble(
  ~Unit, ~`$Y_i^*$`, ~`$R_i$`, ~`$Y_i$`,
  1, 1, 1, "1",
  2, -99, 0, "$\\widehat E[Y_i] = 3/4$",
  3, 1, 1, "1",
  4, 0, 1, "0",
  5, 1, 1, "1",
  6, -99, 0, "$\\widehat E[Y_i] = 3/4$"
)


tt(tab3) %>% 
  style_tt(i = c(2, 6),
           j = c(2, 4),
           color = "purple")
```

Which is equivalent to imputing missing obs with the sample mean

## MAR

```{r, echo = FALSE}

tt(tab1) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") 
```

. . .

We assume *conditional independence*

## MAR

```{r, echo = FALSE}

tt(tab1) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") 
```

Imagine we now observe covariate $X_i$


## MAR

```{r, echo = FALSE}
tab4 = tribble(
  ~Unit, ~`$Y_i^*$`, ~`$R_i$`, ~`$X_i$`,
  1, 1, 1, 0,
  2, -99, 0, 0,
  3, 1, 1, 0,
  4, 0, 1, 0,
  5, 1, 1, 1,
  6, -99, 0, 1
)

tt(tab4) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") 
```

Imagine we now observe covariate $X_i$

## MAR

```{r, echo = FALSE}
tt(tab4) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") 
```

MAR lets us impute the sample mean of units with the same covariate values

## MAR

```{r, echo = FALSE}
tab5 = tribble(
  ~Unit, ~`$Y_i^*$`, ~`$R_i$`, ~`$X_i$`, ~`$Y_i$`,
  1, 1, 1, 0, "1",
  2, -99, 0, 0, "$\\widehat E[Y_i|X_i=0] = 2/3$",
  3, 1, 1, 0, "1",
  4, 0, 1, 0, "0",
  5, 1, 1, 1, "1",
  6, -99, 0, 1, "$\\widehat E[Y_i|X_i=1] = 1$"
)

tt(tab5) %>% 
  style_tt(i = c(2, 6),
           j = c(2, 5),
           color = "purple") 
```

We can impute the sample mean of units with the same covariate values

## MAR with multiple covariates

```{r, echo = FALSE}

tt(tab1) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") 
```

Imagine we observe covariates $X_{1i}$ and $X_{2i}$

## MAR with multiple covariates

```{r, echo = FALSE}
tab6 = tribble(
  ~Unit, ~`$Y_i^*$`, ~`$R_i$`, ~`$X_{1i}$`, ~`$X_{2i}$`,
  1, 1, 1, 0, 3,
  2, -99, 0, 0, 7,
  3, 1, 1, 0, 9,
  4, 0, 1, 0, 5,
  5, 1, 1, 1, 4,
  6, -99, 0, 1, 3
)

tt(tab6) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") 
```

Imagine we observe covariates $X_{1i}$ and $X_{2i}$

## Options

1. Regression estimation

2. Hot deck imputation

3. Inverse-probability weighting (IPW)

## Regression estimation

```{r, echo = FALSE}
tt(tab6) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") 
```


. . .

Estimate $\widehat Y_i^* = \widehat \beta_0 + \widehat \beta_1 X_{1i} + \widehat \beta_2 X_{2i}$ with observed data

## Regression estimation

```{r, echo = FALSE}
tt(tab6) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") 
```

Use coefficients to impute missing values

## Regression estimation

```{r, echo = FALSE}
tab7 = tribble(
  ~Unit, ~`$Y_i^*$`, ~`$R_i$`, ~`$X_{1i}$`, ~`$X_{2i}$`, ~`$Y_i$`,
  1, 1, 1, 0, 3, "1",
  2, -99, 0, 0, 7, "$\\widehat \\beta_0 + \\widehat \\beta_1 \\cdot 0 + \\widehat \\beta_2 \\cdot 7$",
  3, 1, 1, 0, 9, "1",
  4, 0, 1, 0, 5, "0",
  5, 1, 1, 1, 4, "1",
  6, -99, 0, 1, 3, "$\\widehat \\beta_0 + \\widehat \\beta_1 \\cdot 1 + \\widehat \\beta_2 \\cdot 3$"
)

tt(tab7) %>% 
  style_tt(i = c(2, 6),
           j = c(2, 6),
           color = "purple") 
```

. . .

Which yields the same $\widehat E[Y_i]$ as

## Regression estimation

```{r, echo = FALSE}
tab8 = tribble(
  ~Unit, ~`$Y_i^*$`, ~`$R_i$`, ~`$X_{1i}$`, ~`$X_{2i}$`, ~`$Y_i$`,
  1, 1, 1, 0, 3, "$\\widehat \\beta_0 + \\widehat \\beta_1 \\cdot 0 + \\widehat \\beta_2 \\cdot 3$",
  2, -99, 0, 0, 7, "$\\widehat \\beta_0 + \\widehat \\beta_1 \\cdot 0 + \\widehat \\beta_2 \\cdot 7$",
  3, 1, 1, 0, 9, "$\\widehat \\beta_0 + \\widehat \\beta_1 \\cdot 0 + \\widehat \\beta_2 \\cdot 9$",
  4, 0, 1, 0, 5, "$\\widehat \\beta_0 + \\widehat \\beta_1 \\cdot 0 + \\widehat \\beta_2 \\cdot 5$",
  5, 1, 1, 1, 4, "$\\widehat \\beta_0 + \\widehat \\beta_1 \\cdot 1 + \\widehat \\beta_2 \\cdot 4$",
  6, -99, 0, 1, 3, "$\\widehat \\beta_0 + \\widehat \\beta_1 \\cdot 1 + \\widehat \\beta_2 \\cdot 3$"
)

tt(tab8) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple") %>% 
  style_tt(i = 1:6,
           j = 6,
           color = "purple")
```

. . .

What if we could condition on a *function* of covariates instead?

## Propensity score

. . .

**Response propensity function**

$$
p_R(\mathbf{x}) = Pr[R_i = 1 | \mathbf{X}_i = x]
$$

::: incremental
- $p_R(\mathbf{X}_i)$ is the **propensity score** of unit $i$

- Conditional probability of response given the covariates

- Useful since you cannot condition on many covariates  *nonparametrically* 

- Can estimate *consistently* via ML (usually logit)

- Still need to assume *specification* is correct
:::

## Hot deck imputation

```{r, echo = FALSE}
pstab = tribble(
  ~Unit, ~`$Y_i^*$`, ~`$R_i$`, ~`$X_{1i}$`, ~`$X_{2i}$`,
  1, 2, 1, 0, 3,
  2, -99, 0, 0, 7,
  3, 3, 1, 0, 9,
  4, 10, 1, 0, 5,
  5, 12, 1, 1, 4,
  6, -99, 0, 1, 3
) 

tt(pstab) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple")
  
```

. . .

Add propensity score

## Hot deck imputation

```{r, echo = FALSE}
pstab$`$p_R(\\mathbf{X}_i)$` = c(0.33, 0.14, 0.73, 0.35, 0.78, 0.70)

tt(pstab) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple")
```

Add propensity score

## Hot deck imputation

```{r, echo = FALSE}


tt(pstab) %>% 
  style_tt(i = c(2, 6),
           j = 2,
           color = "purple")
```

Impute values with nearest propensity score

## Hot deck imputation

```{r, echo = FALSE}
pstab$`$Y_i$` = c(
  "2",
  "$\\widehat E[Y_i|p_R(X_i) = 0.14]$",
  "3",
  "10",
  "12",
  "$\\widehat E[Y_i|p_R(X_i) = 0.70]$"
)

tt(pstab) %>% 
  style_tt(i = c(2, 6),
           j = c(2,7),
           color = "purple")
```

Impute values with nearest propensity score

## Hot deck imputation

```{r, echo = FALSE}
pstab$`$Y_i$` = c(2, 2, 3, 10, 12, 3)

tt(pstab) %>% 
  style_tt(i = c(2, 6),
           j = c(2,7),
           color = "purple")
```

Impute values with nearest propensity score

## Hot deck what?

![](https://www.hnf.de/uploads/tx_templavoila/Kartenlocher_HNF_1.JPG){fig-align="center"}



## Hot deck imputation

```{r, echo = FALSE}
pstab$`$Y_i$` = c(2, 2, 3, 10, 12, 3)

tt(pstab) %>% 
  style_tt(i = c(2, 6),
           j = c(2,7),
           color = "purple")
```

Impute values with nearest propensity score

## Hot deck imputation

```{r, echo = FALSE}
tt(pstab) %>% 
  style_tt(i = c(2, 6),
           j = c(2,7),
           color = "purple")
```

**Not a good idea.** Discards a lot of information!

## Inverse probability weighting (IPW)

::: incremental
- The problem is not missing data

- But an unrepresentative sample

- If MAR holds, we can use propensity score to *weight* observations

- **Inverse probability weight** $\frac{1}{p_R(\mathbf{X}_i)}$

:::

## IPW estimator


**Horvitz-Thompson estimator**

$$
\widehat E_{IPW}[Y_i] = \frac{1}{n} \sum_{i=1}^n \frac{Y_i^* R_i}{\widehat p_R(\mathbf{X}_i)}
$$

::: incremental

- Low propensity scores are *underrepresented* $\rightarrow$ Assign greater weight

- High-propensity scores $\rightarrow$ lower weight

:::

::: aside
In practice, HÃ¡jek's stabilized IPW estimator is used more often because it avoids disproportional weights (cf. Definition 6.2.8 in AM Chapter 6)
:::

## Recap

. . .

**Options under MAR**

1. Regression estimation

2. Hot deck imputation

3. Inverse-probability weighting (IPW)

## Recap

**Options under MAR**

1. Regression estimation

2. ~~Hot deck imputation~~

3. Inverse-probability weighting (IPW)

## Recap

**Options under MAR**

1. Regression estimation

2. ~~Hot deck imputation~~

3. Inverse-probability weighting (IPW)

4. **Doubly robust estimation**

## Doubly robust estimation

::: incremental
- Regression specification may be incorrect

- Propensity score specification may be incorrect

- Combining the two in a model that yields consistent estimates when *either* is correct
:::

. . .

$$
\begin{align}
& \widehat E_{DR}[Y_i] = \\
&\frac{1}{n} \sum_{i=1}^n \widehat E[Y_i^*|R_i = 1, \mathbf{X}_i] + \frac{1}{n} \sum_{i=1}^n \frac{R_i(Y_i^* - \widehat E[Y_i^* | R = 1, \mathbf{X}_i])}{\widehat{p}_R(\mathbf{X}_i)}
\end{align}
$$

## Doubly robust estimation

- Regression specification may be incorrect

- Propensity score specification may be incorrect

- Combining the two in a model that yields consistent estimates when *either* is correct


$$
\widehat E_{DR}[Y_i] = \text{Regression estimator} + \text{IPW correction}
$$

## With data





