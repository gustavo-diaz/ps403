---
format: 
  revealjs:
    slide-number: false
    progress: false
    chalkboard: true
    code-overflow: wrap
    code-line-numbers: false
---


```{r setup, include=FALSE}
# Global options for the rendering behavior of all subsequent code chunks
knitr::opts_chunk$set(echo = FALSE, 
                      warning = FALSE,
                      message = FALSE,
                      fig.pos = "center")

library(tidyverse)
library(tinytable)
library(kableExtra) # too lazy to change
library(DeclareDesign) # Re-randomize
library(infer)

# tinytable options
# options(tinytable_html_mathjax = TRUE)
```



## Making causal claims

&nbsp;

. . .

:::{.r-stack}
How do we know $X$ *causes* $Y$?
:::

&nbsp;

. . .

:::{.r-stack}
Can data help us make causal claims?
:::

&nbsp;

. . .

:::{.r-stack}
When is a causal claim *not* appropriate?
:::

## The lady tasting tea

. . . 

*A lady declares that by tasting a cup of tea made with milk she can discriminate whether the milk or the tea infusion was first added to the cup*

::: aside
Adapted from Fisher, R.A. 1935. [*The design of experiments*](https://mimno.infosci.cornell.edu/info3350/readings/fisher.pdf). Oliver & Boyd. Chapter 2
:::


&nbsp;

::: incremental
- Is this a *causal* claim?
- How do you evaluate it?
:::

## An experiment

::: incremental
- Suppose we have eight milk tea cups

- 4 milk first, 4 tea first

- We arrange them in random order

- Lady knows there are 4 of each, but not which ones
:::

## Results

```{r}
tea = tribble(
  ~Lady, ~Tea, ~Milk,
  "Tea", 3, 1,
  "Milk", 1, 3
)

tea = tea %>% mutate(
  Lady = ifelse(Lady == "Tea", 
                "Tea First",
                "Milk First"))

colnames(tea) = c("Lady's Guesses", "Tea First", "Milk First")

tea %>% 
  kbl() %>% 
  add_header_above(c(" " = 1, "True Order" = 2))
```

::: incremental
- Lady gets it right $6/8$ times

- What can we conclude?
:::

## Problem

::: incremental
- How does "being able to discriminate" look like?

- We **do know** how a person *without* the ability to discriminate milk/tea order looks like

- This is our **null hypothesis** ($H_0$)

- Which lets us make **probability statements** about this **hypothetical world of no effect** 
:::

## A person with no ability

```{r}
nulldist = tribble(
  ~count, ~combinations, ~number,
  0, "xxxx", "\\(1 \\times 1 = 1\\)",
  1, "xxxo, xxox, xoxx, oxxx", "\\(4 \\times 4 = 16\\)", 
  2, "xxoo, xoxo, xoox, oxox, ooxx, oxxo", "\\(6 \\times 6 = 36\\)",
  3, "xooo, oxoo, ooxo, ooox", "\\(4 \\times 4 = 16\\)",
  4, "oooo", "\\(1 \\times 1 = 1\\)"
)

colnames(nulldist) = c("Count", "Possible combinations", "Total")

nulldist %>% 
  kbl(escape = FALSE) %>% 
  column_spec(3, color = "white")
  
```

::: aside
Ways of getting a number of tea-first cups right
:::

. . .

- This is a symmetrical problem!

## A person with no ability {visiblity="uncounted"}

```{r}
nulldist = tribble(
  ~count, ~combinations, ~number,
  0, "xxxx", "\\(1 \\times 1 = 1\\)",
  1, "xxxo, xxox, xoxx, oxxx", "\\(4 \\times 4 = 16\\)", 
  2, "xxoo, xoxo, xoox, oxox, ooxx, oxxo", "\\(6 \\times 6 = 36\\)",
  3, "xooo, oxoo, ooxo, ooox", "\\(4 \\times 4 = 16\\)",
  4, "oooo", "\\(1 \\times 1 = 1\\)"
)

colnames(nulldist) = c("Count", "Possible combinations", "Total")

nulldist %>% 
  kbl(escape = FALSE) %>% 
  column_spec(3, color = "white")
  
```

::: aside
Ways of getting a number of milk-first cups right
:::


## A person with no ability {visiblity="uncounted"}

```{r}
nulldist %>% 
  kbl(escape = FALSE)
```

. . .

- A person guessing at random gets $6/8$ cups right with probability $\frac{16}{70} \approx 0.23$


::: aside
Ways of getting a number of tea-first **and** milk-first cups right
:::

## A person with no ability {visiblity="uncounted"}

```{r}
nulldist %>% 
  kbl(escape = FALSE)
```

- And **at least** $6/8$ cups with $\frac{16 + 1}{70} \approx 0.24$


::: aside
Ways of getting a number of tea-first **and** milk-first cups right
:::

## p-values

::: incremental
- If the lady is **not** able to discriminate milk-tea order, the probability of observing $6/8$ correct guesses or better is $0.24$

- A **p-value** is the probability of observing a result *equal or more extreme* than what is originally observed...

- ... *when* the **null hypothesis** is true

- Smaller p-values give more evidence **against** the null

- Implying observed value is *less likely to have emerged by chance*

:::

::: aside
This is Fisher's interpretation of p-values, which is closest to modern day NHST. [Neyman and Pearson had different ideas](https://pmc.ncbi.nlm.nih.gov/articles/PMC4347431/) about hypothesis testing
:::

## Rules of thumb

::: incremental

- A convention in the social sciences is to claim that something with $p < 0.05$ is *statistically significant*

- Meaning we have enough evidence 

- Committing to a **significance level** $\alpha$ implies accepting that sometimes we will get $p < 0.05$ by chance

- This is a **false positive** result

::: aside
No good reason for $\alpha = 0.05$ other than path dependency.
:::

:::

## Types of error

```{r}
error_types = tribble(
  ~Decision, ~`\\(H_0\\) true`, ~`\\(H_0\\) not true`,
  "Don't reject \\(H_0\\)", "True negative", "False negative (type II error)",
  "Reject \\(H_0\\)", "False positive (type I error)", "True positive"
)



error_types %>% 
  kbl(escape = FALSE) %>% 
  column_spec(1, bold = TRUE, border_right = TRUE) %>% 
  column_spec(2:3, width = "8cm") %>% 
  add_header_above(c(" ", "Unobserved reality" = 2))
```

## Hypothesis testing

::: incremental
- We just computed p-values via *permutation* testing


- Congenial with *agnostic statistics* because we do not need to assume anything beyond how the data was collected

- Being *agnostic* helps us add credibility to our **causal claims**
:::

::: aside
Normal approximation also works with large enough samples
:::

## Potential outcomes framework

. . .

**Notation**

::: incremental
- $D_i = \{0,1\}$ condition (0: control, 1: treatment)

- $Y_i(D_i)$ is the individual **potential outcome**
:::

. . .

$$
Y_i = \begin{cases}
Y_i(0) & : & D_i = 0 \\
Y_i(1) & : & D_i = 1
\end{cases}
$$

. . .

Switching equation

$$
Y_i = Y_i(1) D_i + Y_i(0) (1 - D_i)
$$

## Toy example

```{r}
pop = declare_population(
  N = 4,
  female = c(0, 0, 1, 1),
  Y0 = c(0, 0, 0, 1),
  Y1 = c(0, 1, 1, 1)
)

pot = declare_potential_outcomes(Y ~ Y1 * Z + Y0 * (1-Z))

estimand = declare_inquiry(ATE = mean(Y1 - Y0))

assign = declare_assignment(Z = complete_ra(N, m = 2))

reveal = declare_measurement(Y = reveal_outcomes(Y ~ Z))

estimator = declare_estimator(Y ~ Z, inquiry = "ATE")

design_1 = pop + pot + estimand + assign + reveal + estimator
```

```{r}
set.seed(142)

dat = draw_data(design_1)

dat_0 = dat %>% select(ID, Y1, Y0)

colnames(dat_0) = c("ID", "\\(Y_i(1)\\)", "\\(Y_i(0)\\)")

dat_0 %>% kbl(escape = FALSE)
```

. . .

$\tau_i = Y_i(1) - Y_i(0)$ is the **unit-level treatment effect**

&nbsp;

## Toy example

```{r}
dat_1 = dat %>% 
  select(ID, Y1, Y0) %>% 
  mutate(tau = Y1 - Y0)

colnames(dat_1) = c("ID", "\\(Y_i(1)\\)", "\\(Y_i(0)\\)", "\\(\\tau_i\\)")

dat_1 %>% kbl(escape = FALSE)
```

$\tau_i = Y_i(1) - Y_i(0)$ is the **unit-level treatment effect**

&nbsp;

. . .



$E[\tau_i] = \frac{1}{n} \sum_{i=1}^n \tau_i$ is the  **average treatment effect** (ATE)

## Assumption

**Stable Unit Treatment Value (SUTVA)**

. . .

Potential outcomes are *stable*, meaning:

::: incremental
1.  *No unobserved multiple versions of the treatment:* Treatment is the same for everyone

2. *No interference:* Unit $i$'s potential outcome is not affected by $j$'s potential outcome
:::



## Challenge


```{r}
dat_1 %>% 
  kbl(escape = FALSE)
```


## Challenge

```{r}
dat_1 %>% 
  kbl(escape = FALSE) %>% 
  add_header_above(c(" ", "Unobserved" = 3))
```

. . .

Assign condition $D_i$ (0: control, 1: treatment)

## Challenge

```{r}
dat_2 = dat %>% 
  select(ID, Y1, Y0, Z, Y) %>% 
  mutate(tau = Y1 - Y0) %>% 
  relocate(tau, .after = "Y0")

colnames(dat_2) = c("ID", "\\(Y_i(1)\\)", "\\(Y_i(0)\\)", "\\(\\tau_i\\)",
                    "\\(D_i\\)", "\\(Y_i\\)")

dat_2 %>% 
  kbl(escape = FALSE) %>%
  add_header_above(c(" ", "Unobserved" = 3,  "Observed" = 2))
```

. . .

To know $E[\tau_i]$ we need $(Y_i(1) - Y_i(0))$

. . .

But we only observe one at a time for each unit

## Challenge

```{r}
dat_2 = dat %>% 
  select(ID, Y1, Y0, Z, Y) %>% 
  mutate(tau = Y1 - Y0) %>% 
  relocate(tau, .after = "Y0")

colnames(dat_2) = c("ID", "\\(Y_i(1)\\)", "\\(Y_i(0)\\)", "\\(\\tau_i\\)",
                    "\\(D_i\\)", "\\(Y_i\\)")

dat_2 %>% 
  kbl(escape = FALSE) %>%
  add_header_above(c(" ", "Unobserved" = 3,  "Observed" = 2))
```

This is the **FUNDAMENTAL PROBLEM OF CAUSAL INFERENCE**


::: aside
The term comes from: Holland, Paul W. 1986. ["Statistics and Causal Inference."](https://doi.org/10.2307/2289064) *Journal of the American Statistical Association* 81 (396): 945-960
:::

## Challenge

```{r}
dat_2 = dat %>% 
  select(ID, Y1, Y0, Z, Y) %>% 
  mutate(tau = Y1 - Y0) %>% 
  relocate(tau, .after = "Y0")

colnames(dat_2) = c("ID", "\\(Y_i(1)\\)", "\\(Y_i(0)\\)", "\\(\\tau_i\\)",
                    "\\(D_i\\)", "\\(Y_i\\)")

dat_2 %>% 
  kbl(escape = FALSE) %>%
  add_header_above(c(" ", "Unobserved" = 3,  "Observed" = 2))
```

How do we approximate $E[\tau_i]$ with observed data?

## ATE decomposition

$$
E[\tau_i]
$$

. . .

Substitute unit-level treatment effect

## ATE decomposition

$$
\begin{align}
E[\tau_i] & = E[Y_i(1) - Y_i(0)]
\end{align}
$$

. . .

Apply linearity of expectations

## ATE decomposition

$$
\begin{align}
E[\tau_i] & = E[Y_i(1) - Y_i(0)] \\
  & = E[Y_i(1)] - E[Y_i(0)]
\end{align}
$$

. . .

We can only calculate

. . .

$$
E[Y_i(1) | D_i = 1] - E[Y_i(0) | D_i = 0] 
$$

. . .

We need to convince ourselves that

. . .

$$
E[Y_i(1)] - E[Y_i(0)] = E[Y_i(1) | D_i = 1] - E[Y_i(0) | D_i = 0]
$$

## ATE decomposition

$$
\begin{align}
E[\tau_i] & = E[Y_i(1) - Y_i(0)] \\
  & = E[Y_i(1)] - E[Y_i(0)]
\end{align}
$$


We can only calculate


$$
E[Y_i(1) | D_i = 1] - E[Y_i(0) | D_i = 0] 
$$


We need to convince ourselves that

$$
E[Y_i(1)] - E[Y_i(0)] = E[Y_i(1) \color{purple}{| D_i = 1}] - E[Y_i(0) \color{purple}{| D_i = 0}]
$$


## ATE decomposition

$$
\begin{align}
E[\tau_i] & = E[Y_i(1) - Y_i(0)] \\
  & = E[Y_i(1)] - E[Y_i(0)]
\end{align}
$$


We can only calculate


$$
E[Y_i(1) | D_i = 1] - E[Y_i(0) | D_i = 0] 
$$


We need to convince ourselves that [$D_i$ can be ignored]{style="color: purple;"}

$$
E[Y_i(1)] - E[Y_i(0)] = E[Y_i(1) \color{purple}{| D_i = 1}] - E[Y_i(0) \color{purple}{| D_i = 0}]
$$

## Obstacle

$$
E[Y_i(1) | D_i = 1] - E[Y_i(0) | D_i = 0]
$$


## Obstacle

$$
E[\color{purple}{Y_i(1)} | D_i = 1] - E[Y_i(0) | D_i = 0]
$$

. . .

Rewrite to include $\tau_i$

. . .


$$
= E[\color{purple}{\tau_i + Y_i(0)} | D_i = 1] - E[Y_i(0) | D_i = 0]
$$

. . .

Apply linearity of expectations again


$$
= E[\color{purple}{\tau_i} | D_i = 1] +
E[\color{purple}{Y_i(0)} | D_i = 1] - 
E[Y_i(0) | D_i = 0]
$$


## Obstacle

$$
E[\color{purple}{Y_i(1)} | D_i = 1] - E[Y_i(0) | D_i = 0]
$$



Rewrite to include $\tau_i$




$$
= E[\color{purple}{\tau_i + Y_i(0)} | D_i = 1] - E[Y_i(0) | D_i = 0]
$$



Apply linearity of expectations again


$$
= \underbrace{E[\color{purple}{\tau_i} | D_i = 1]}_{\text{ATE on the treated (ATT)}} +
\underbrace{E[\color{purple}{Y_i(0)} | D_i = 1] - E[Y_i(0) | D_i = 0]}_{\text{Selection bias}}
$$

. . .


We want to eliminate **selection bias**

## Strategy 1: Random assignment

. . .

If treatment $D_i$ is *randomly assigned*

::: incremental
- Potential outcomes are independent from treatment: $(Y_i(0), Y_i(1)) \perp \!\!\! \perp D_i$
- Positivity: $0 < \text{Pr}[D_i = 1] < 1$
- ATE $E[\tau_i]$ is **point-identified**
- Estimate with difference in means between treatment and control: $\widehat E_{DM}[\tau_i] = \widehat E[Y_i(1)] - \widehat E[Y_i(0)]$
:::

. . .

```{r, echo = TRUE, eval = FALSE}
difference_in_means(Y ~ D, data = data) # estimatr package
```

## Randomization inference

::: incremental
1. Choose a **test statistic** and **sharp null hypothesis**

2. Calculate *observed* test statistic $T^{obs}$

3. Randomly shuffle treatment assignment

4. Calculate test statistic $\widetilde T_1$

5. Repeat $K$ times to obtain distribution of test statistics $\widetilde T = \{\widetilde T_1, \ldots, \widetilde T_K \}$

6. **p-value:**  Proportion in $\widetilde T >= T^{obs}$
:::

::: aside
The **sharp null hypothesis** assumes no effect at all. $H_0 : \tau_i = Y_i(1) - Y_i(0) = 0 \forall i$
:::

## Example

```{r}
declaration = 
  declare_model(N = 100, U = rnorm(N),
                potential_outcomes(Y ~ 0.25 * Z + U)) +
  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) +
  declare_assignment(Z = complete_ra(N, prob = 0.5)) +
  declare_measurement(Y = reveal_outcomes(Y ~ Z)) +
  declare_estimator(Y ~ Z, model = difference_in_means, 
                    inquiry = "ATE")

set.seed(20241108)
experiment = draw_data(declaration) %>% 
  rename(D = Z) %>% 
  select(ID, Y, D) 
```


```{r, echo = TRUE}
experiment
```


## Create randomization distribution

::: aside
Code relies on `tidyverse`, `infer` and `estimatr` packages
:::

::: {.panel-tabset}
## Code

```{r, echo = TRUE}
#| code-line-numbers: "|3|4|5|6|7|8|9|10"
set.seed(20241108)

null_df = experiment %>% 
  specify(Y ~ D) %>% 
  hypothesize(null = "independence") %>% 
  generate(reps = 1000, type = "permute") %>% 
  split(.$replicate) %>% 
  map(~difference_in_means(Y ~ D, data = .)) %>% 
  map(tidy) %>% 
  bind_rows(.id = "replicate")
```

## Output

```{r}
null_df
```

:::

## Visualize

```{r}
obs = difference_in_means(Y ~ D, data = experiment) %>%
  tidy %>% .$estimate

dens = density(null_df$estimate)

data = tibble(estimate = dens$x,
              dens = dens$y) %>% 
  mutate(p = ifelse(abs(estimate) >= abs(obs), 1, 0))

p0 = ggplot(data) + 
  aes(x = estimate,
      y = dens) + 
  geom_line(linewidth = 2) +
  labs(x = "ATEs under sharp null hypothesis",
       y = "Density") +
  theme_gray(base_size = 20)

p0
  
```

## Visualize

```{r}

p1 = p0 +
  geom_vline(
    xintercept = c(-obs, obs),
    linetype = "dashed",
    color = "purple",
    linewidth = 1) +
  annotate(
    "text",
    x = obs + 0.15,
    y = 1.8,
    label = "Observed ATE",
    color = "purple"
    )

p1
```

## Visualize

```{r}
p2 = p1 +
  geom_area(
    data = filter(data, p == 1 & estimate > 0),
    fill = "purple",
    alpha = 0.5
  ) +
  geom_area(
    data = filter(data, p == 1 & estimate < 0),
    fill = "purple",
    alpha = 0.5
  ) 
  
p2
```

## Visualize

```{r}
p2
```


```{r, echo = TRUE}
mean(abs(null_df$estimate) >= abs(obs))
```

## Normal-approximation inference

```{r, echo = TRUE}
mean(abs(null_df$estimate) >= abs(obs)) # randomization inference p-value
```

. . .

Compare with:

```{r, echo = TRUE}
difference_in_means(Y ~ D, data = experiment) %>% tidy()  %>% .$p.value
```

::: incremental
- Normal approximation compares test statistic to t-distribution

- Based on "weak" null hypothesis $H_0: E[\tau_i] = 0$

- vs. randomization inference sharp null $H_0 : \tau_i = Y_i(1) - Y_i(0) = 0 \forall i$
:::


## Strategy 2: Ignorability

. . .

What if we cannot assume random assignment?

## Strategy 2: Ignorability

We can still conduct valid causal inference if we believe treatment assignment is **strongly ignorable** (conditional on covariates)

. . .

Requirements:

::: incremental
- *Conditional independence*: $(Y_i(0), Y_i(1)) \perp \!\!\! \perp D_i | \mathbf{X}_i$
- *Positivity*: There exists $\epsilon > 0$ such that $$\epsilon < Pr[D_i = 1 | \mathbf{X}_i] < 1 - \epsilon$$
:::

::: aside
*Weak ignorability* only requires $Y_i(d) \perp \!\!\! \perp D_i | \mathbf{X}_i \forall d \in \{0, 1\}$, which in most cases makes no difference
:::

## Strategy 2: Ignorability

If strong ignorability holds:

::: incremental
- ATE $E[\tau_i]$  is **point-identified**
- Conditional ATE (CATE) $E[\tau_i | \mathbf{X}_i]$ is **point-identified**
:::

. . .

Estimation strategies (more in the lab):

::: incremental
- Conditioning on covariates (regression)
- Propensity score matching
- IPW estimation
- Doubly robust estimation
:::

## Next week: Causal Inference II

**General topic:** Satisfying ignorability by design.

- Regression discontinuity

- Difference-in-differences

**Focus:** How do they satisfy ignorability?

**Just for reference:** Methodological critiques

&nbsp;

:::{.r-stack}
CHECK UPDATED SYLLABUS ON GITHUB!
:::