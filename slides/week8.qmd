---
title: "Parametric Models"
subtitle: "POLI_SCI 403: Probability and Statistics"
format: 
  revealjs:
    slide-number: false
    progress: false
    code-overflow: wrap
    chalkboard: true
---


```{r setup, include=FALSE}
# Global options for the rendering behavior of all subsequent code chunks
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE,
                      message = FALSE,
                      fig.pos = "center")


```

# Agenda

- Main idea
- Parametric regression
- Binary choice models (logistic regression)
- Maximum likelihood estimation
- Lab

# Parametric Models

## So far 

- **Agnostic statistics approach:** Assume only what can be gleaned from data

- No need to make *distributional* assumptions, only i.i.d. process

- Meaning our approach is *non-parametric*

- **Alternative:** Parametric models

## What does "parametric" mean?

::: incremental
- **Parametric:** Assume full functional form with finite number of parameters

- **Nonparametric:** Assume functional form is unknown, possibly infinite number of parameters

- **Semiparametric:** ¯\\_(ツ)_/¯
:::

## Ingredients of a parametric model

::: incremental
- Set of functions $g$
- Indexed by parameter vector $\theta$
- Model is true for every $\theta$ if...
- $f_{Y|X}(y|x) = g(y,x; \theta)$
:::

. . .

We assume there exists a function $g$ with parameters $\theta$

. . .

If you know $\theta$, then you can *fully characterize* the PMF/PDF of $Y$ given $X$

. . .

Then we can say $\theta$ is a *sufficient statistic*

## Toy example: Biased coin flip

. . .

$$
f_Y(y) = g(y;p) = 
  \begin{cases}
  1-p &:& y = 0\\
  p &:& y=1\\
  0 &:& \text{otherwise.}
  \end{cases}
$$

::: incremental
- $\theta = p$
- If you know $p$, then you fully know the distribution of random variable $Y$
:::

# Regression

::: {style="font-size: 40%;"}
again
:::


## Classical linear model

. . .

$$
Y = \boldsymbol{X \beta} + \varepsilon
$$

::: incremental
- $Y$: Response (Outcome)
- $X$: Matrix of predictors (explanatory variables)
- $\boldsymbol{\beta} = (\beta_0, \beta_1, \ldots, \beta_K)^T$ Vector of coefficients
- $\varepsilon \sim N(0, \sigma^2)$ Errors (residuals) i.i.d. normal with expectation zero 
:::

. . .

If you know $\boldsymbol{\beta}$ and $\sigma^2$, then you can fully characterize the PMF/PDF of $Y$ given $X$

## Why do we need $\varepsilon \sim N(0, \sigma^2)$?

::: incremental
- $\boldsymbol{\beta}$ represents the variation in $Y$ that comes from predictors $\boldsymbol{X}$
- $\varepsilon$ represents variation in $Y$ that **cannot** be attributed to $\boldsymbol{X}$
- If errors are i.i.d. then they are also *independent* of $\boldsymbol{X}$
- We need this so that our *estimator* is **unbiased** by definition
:::

. . .

$$
Y = \beta_0 + \beta_1 X_1 + \varepsilon
$$

. . .

$$
Y = \beta_0 + \beta_1 X_1 + \color{purple}{\beta_2 X_2} + \varepsilon
$$

::: aside
[Dr. Huntington-Klein explains it better](https://youtu.be/0Aukw3CdB-Q?si=A6mQhm2l9QE3ObHM)
:::

# Binary Choice Models

## Logistic regression

::: aside
Called logit model in the AM textbook
:::

. . .

Parametric model

$$
g(y, \boldsymbol{X}; \beta) = 
\begin{cases}
1-h(\boldsymbol{X\beta}) &:& y = 0\\
h(\boldsymbol{X\beta}) &:& y = 1 \\
0 &:& \text{otherwise.}
\end{cases}
$$

. . .

Let $h(\boldsymbol{X\beta}) = Pr(Y = 1 | X) = p(X)$ for simplicity

## Logistic regression

::: aside
Called logit model in the AM textbook
:::

Parametric model

$$
g(y, \boldsymbol{X}; \beta) = 
\begin{cases}
1-h(\boldsymbol{X\beta}) &:& y = 0\\
h(\boldsymbol{X\beta}) &:& y = 1 \\
0 &:& \text{otherwise.}
\end{cases}
$$

## Logistic regression

::: aside
Called logit model in the AM textbook
:::

Parametric model

$$
g(y, \boldsymbol{X}; \beta) = 
\begin{cases}
1-p(X) &:& y = 0\\
p(X) &:& y = 1 \\
0 &:& \text{otherwise.}
\end{cases}
$$

::: incremental
- $\boldsymbol{\beta} = (\beta_0, \beta_1, \ldots, \beta_K)^T$ 
- $p(X)$ or $h(\boldsymbol{X\beta})$ is the *mean* or *link* function
- We need the link to bound $y$ in the $[0,1]$ range
:::

## Logistic regression


<!-- My old cheatsheet -->
<!-- https://drive.google.com/drive/folders/1y0SwNYAjG5_uDGj7MaaJJoS3TpZ_5HuL -->

For the logit model, the link is the *logistic function*

$$
p(X) = \frac{e^{X\beta}}{1+e^{X\beta}}
$$

. . .


```{r, echo = FALSE, fig.width=5, fig.height=4, fig.align='center'}
library(tidyverse)

theme_set(theme_gray(base_size = 25))

ggplot(data = data.frame(x = c(-3,3))) +
  aes(x) +
  stat_function(fun = function(x) exp(x)/(1+exp(x)), n = 100, linewidth = 2) +
  ylim(0,1) +
  labs(y = "p(X)")
  
```

## Logistic regression


For the logit model, the link is the *logistic function*

$$
p(X) = \frac{e^{X\beta}}{1+e^{X\beta}}
$$

. . .

Rearrange to get the *odds ratio*

$$
\frac{p(X)}{1-p(X)} = e^{{X\beta}}
$$

::: aside
This is still a **linear model** in the sense that it is *linear in parameters*
:::


## Logistic regression


Taking the natural logarithm gives the *log odds*

$$
log \left (\frac{p(X)}{1-p(X)} \right) = X\beta
$$

::: incremental
- Weird to interpret, easy to estimate
- It's called *logit* because you need to *log* *it* to make it easier to estimate
- How do we estimate?
:::

::: aside
Notice we are not making assumptions about an error term because we are modeling a Bernoulli distribution
:::

## Maximum likelihood estimation (MLE)

::: aside
[Dr. King explains it better](https://youtu.be/P79af1fkUsk?si=9eIT50q7KK0uh2nx&t=1313)
:::

<!-- Show intuition with math, then example from ISLR -->
::: incremental
- Knowing $\theta$ is enough to characterize conditional PMF of $Y$ given $X$
- But we do not know $\theta$! We only know the data we observe
- We can try many different values for $\theta$ and see what sticks
- **Trick:** Find $\theta$ that *would have maximized the probability of obtaining the data we observe*
- **Restated:** Find *parameters* that would have made outcomes *as likely as possible*
:::

## Example: Which line makes observed data more likely?

![](https://scholar.harvard.edu/sites/scholar.harvard.edu/files/styles/os_files_xxlarge/public/ellaudet/files/the_least_squares_method_new.gif)

## More formally

. . .

Likelihood function $\mathcal{L}(t|Y, \boldsymbol{X})$

. . .

Maximum likelihood estimator

$$
\widehat{\theta}_{ML} = \underset{t\in\Theta}{\text{argmax}}\mathcal{L}(t|Y, \boldsymbol{X}) = \underset{t\in\Theta}{\text{argmax}} \prod_{i=1}^n\mathcal{L}(t|Y_i, \boldsymbol{X_i})
$$

. . .

Products are hard, maximize *log-likelihood* instead

$$
\widehat{\theta}_{ML} = \underset{t\in\Theta}{\text{argmax}}[\text{log}\mathcal{L}(t|Y, \boldsymbol{X})] = \underset{t\in\Theta}{\text{argmax}} \sum_{i=1}^n\text{log}\mathcal{L}(t|Y_i, \boldsymbol{X_i})
$$

## Another way to look at it

![](fig/mle_viz.png){fig-align="center"}

## How do you find the maximum likehihood?

. . .

**Analytically**

::: incremental
- Take the derivative of $\text{log}\mathcal{L}(\theta|Y, X)$ w.r.t. $\theta$

- Set to $0$, solve for $\theta$ and label it $\widehat \theta$ (if possible)

- Check if second derivative is negative (so it is maximum not minimum)
:::

## How do you find the maximum likehihood?

**Numerically**

. . .

Computer will do it for you

. . .

Use optimization algorithm to find maxima (if any)

. . .

Computationally intensive to write on your own

. . .

Most MLE methods have fast pre-packaged functions

. . .

**Rule of thumb:** If the algorithm takes too long, don't believe the result!

## With data

```{r}
library(ISLR)

head(Default)
```

. . .

```{r}
Default$default01 = ifelse(Default$default == "Yes", 1, 0)

with(Default, table(default, default01))
```

## Estimation

. . .

Linear probability model

```{r}
ols = lm(default01 ~ student + balance + income, data = Default)

coef(ols) # very small numbers
```


## Estimation

Linear probability model

```{r}
ols = lm(default01 ~ student + balance + income, data = Default)

coef(ols) %>% round(5) # easier to read
```

. . .

Logit model

```{r}
logit = glm(default01 ~ student + balance + income, 
            data = Default, family = binomial)

coef(logit) %>% round(5)
```

. . .

Logit coefficients are expressed in *log-odds*

## Convert to probabilities

```{r}
get_logit_prob = function(fit){
  coefs = coef(fit) # extract coefficients from logit fit
  odds = exp(coefs) # transform log-odds to odds
  prob = odds / (1 + odds) # convert odds to probabilities
  prob = round(prob, 5) # round for easy interpretation
  return(prob)
}
```

. . .

```{r}
get_logit_prob(logit)
```

. . .

```{r}
coef(ols) %>% round(5)
```

## Why so different?

::: {.panel-tabset}
## Plot

```{r, echo = FALSE}
library(marginaleffects)

# Extract predictions
p_ols = plot_predictions(ols,
                 condition = "balance",
                 draw = FALSE)

p_log = plot_predictions(logit,
                         condition = "balance",
                         draw = FALSE)

# Combine and label
pred_df = bind_rows(
  p_ols %>% mutate(Estimator = "OLS"),
  p_log %>% mutate(Estimator = "Logit")
)

# Visualize
ggplot(Default) +
  aes(x = balance, y = default01) +
  geom_jitter(alpha = 0.3, height = 0.1) +
  geom_line(
    data = pred_df,
    aes(x = balance, 
        y = estimate,
        color = Estimator),
    linewidth = 2) +
  scale_color_viridis_d(begin = 0, end = 0.8) +
  labs(x = "Balance",
       y = "Pr(Default)")
```


## Code

```{r, eval = FALSE}
library(marginaleffects)

# Extract predictions
p_ols = plot_predictions(ols,
                 condition = "balance",
                 draw = FALSE)

p_log = plot_predictions(logit,
                         condition = "balance",
                         draw = FALSE)

# Combine and label
pred_df = bind_rows(
  p_ols %>% mutate(Estimator = "OLS"),
  p_log %>% mutate(Estimator = "Logit")
)

# Visualize
ggplot(Default) +
  aes(x = balance, y = default01) +
  geom_jitter(alpha = 0.3, height = 0.1) +
  geom_line(
    data = pred_df,
    aes(x = balance, 
        y = estimate,
        color = Estimator),
    linewidth = 2) +
  scale_color_viridis_d(begin = 0, end = 0.8) +
  labs(x = "Balance",
       y = "Pr(Default)")
```


:::

## Why would you want to do this?

. . .

**Critique:** You can reverse engineer MLE so that your preferred parameters are optimal

. . .

**Achen (2002, ARPS):** Stop making up new parametric models!

. . .

Or, rather, motivate your parametric models with well-laid out formal model

. . .

So you get as close as possible to the **microfoundations**

. . .

**Microfoundations:** Explaining macro-level phenomena by modeling the behavior of individual agents


## MLE finite sample properties


. . .

We said OLS was **BLUE**

. . .

MLE find the **MVUE** instead

## MLE finite sample properties

1. **M**inimum

## MLE finite sample properties

1. **M**inimum **V**ariance

## MLE finite sample properties

1. **M**inimum **V**ariance **U**nbiased

## MLE finite sample properties

1. **M**inimum **V**ariance **U**nbiased **E**stimator (MVUE)

. . .

- **Unbiased:** $E[\widehat \theta] = \theta$

. . .

- **Minimum variance:** Efficiency

. . .

*IF* there is a MVUE, ML will find it

. . .

If *not*, it can still find something good enough

## MLE finite sample properties

1. **M**inimum **V**ariance **U**nbiased **E**stimator (MVUE)

. . .

2. Invariance to Reparameterization

. . .

- If $\widehat \theta$ is the MLE of $\theta$, then for any function $f(\theta)$ its MLE is $f(\widehat \theta)$

. . .

- **Example:** $\widehat \sigma$ is the MLE of $\sigma$, $\widehat \sigma^2$ is the MLE of $\sigma^2$

. . .

- Not true for other modes of inference!

## MLE finite sample properties

1. **M**inimum **V**ariance **U**nbiased **E**stimator (MVUE)

2. Invariance to Reparameterization

. . .

3. Invariance to sampling plans

. . .

- We are making not relyng on i.i.d. sampling to connect $\theta$ and $\widehat \theta$

. . .

- So collecting more data after looking at results is perfectly fine!

## MLE finite sample properties

1. **M**inimum **V**ariance **U**nbiased **E**stimator (MVUE)

2. Invariance to Reparameterization

3. Invariance to sampling plans

::: aside
In textbooks, (2) and (3) are jointly referred to as the **invariance property** of MLE
:::

## Asymptotic properties

. . .

1. **Consistency:** WLLN implies that as $ n \rightarrow \infty$, the sampling distribution of the MLE collapses to a spike over the parameter value

. . .

2. **Asymptotic normality:** CLT implies that as  $n \rightarrow \infty$, the distribution of the standardized MLE converges to $N(0,1)$

. . .

3. **Asymptotic efficiency:** Because MLE is a sufficient statistic by definition

::: aside
**Lab:** MLE will often give the same answers and a non-parametric estimator because the asymptotic properties are the same
:::

## What about inference?

. . .

**Short answer:** Normal-approximation standard errors/CIs/p-values are fine because asymptotic normality usually holds

. . .

**Long answer:**
 
. . .

You can conduct inference via likelihood-ratio tests of between parameters of interest

. . .

But too much work to test all points of interest

. . .

**AM:** Don't take your models too seriously! Ok to be agnostic and compute robust/bootstrapped standard errors (more in the lab)

::: aside
See [here](https://projects.iq.harvard.edu/files/gov2001/files/inferencep_3.pdf) for more notes on likelihood-ratio based inference
:::

## Wrapping up

- Parametric models as alternative way to think about inference

- Makes more sense when i.i.d. is (really truly) untenable, but requires careful justification

- Estimated via MLE (alternative is [method of moments](https://online.stat.psu.edu/stat415/lesson/1/1.4))

- Many different opinions on how to deal with uncertainty

- Most people who prefer the model-based approach rely more on Bayesian statistics than MLE

- Overfitting, micronumerosity, etc. are still problems here