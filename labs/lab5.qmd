---
title: "Lab 5: Maximum likelihood estimation"
author: 
  - FirstName LastName
  - <YourEmail@u.northwestern.edu>
date: November 4, 2024
date-format: "**[D]ue:** MMMM D, YYYY"
format: 
     pdf:
       documentclass: article
fontsize: 12pt
urlcolor: blue
number-sections: true
geometry:
      - left=1in
      - right=1in
      - top=1in
      - bottom=1in
header-includes:
    - \usepackage{setspace}
    - \doublespacing
---

```{r setup, include=FALSE}
# Code chunk options
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE,
                      message = FALSE,
                      tidy = FALSE,
                      tidy.opts=list(width.cutoff=80))

# Packages
library(tidyverse)
library(marginaleffects) # Visualize regression output
library(modelsummary) # make pdf regression tables
library(broom) # tidy regression output
library(car) # Data for the lab
library(mvtnorm) # a package for the multivariate normal distribution
library(sandwich) # robust standard errors

```


# Data

The `Chile` data frame contains observations from a national survey conducted in April and May of 1988 by FLACSO/Chile. This survey happened six months before a plebiscite that would decide the faith of the country's dictatorship at the time. A *yes* vote would have led to eight more years of military rule, a *no* vote would set in motion a democratization process (which is what eventually happened).

```{r, eval = FALSE}
# Explore but do not print in pdf
Chile
```

You can learn more about how each variable is coded by passing `?Chile` or `help(Chile)` to the console.

Our outcome variable is `vote`, which we will recode as a binary outcome called `yes`, which will take a value of 1 is the person indicated they will vote in favor of the dictatorship in the plebiscite, and 0 otherwise.

```{r}
chile = Chile %>% drop_na

chile$yes = ifelse(chile$vote == "Y", 1, 0)

# check
with(chile, table(vote, yes))
```

Our main explanatory variable will be `statusquo`, which is an aggregate index of many questions that ask the respondent whether they agree with political, social, and economic policies implemented by the military regime. Higher scores represent more support for the dictatorship's policies.

```{r}
summary(chile$statusquo)
```

## Explain what each variable in the original `Chile` data frame is measuring

# Classical linear model

Consider the following model:

$$
\text{yes} = \beta_0 + \beta_1 \mathtt{statusquo} + \varepsilon
$$

This a classical linear model, so knowing $\beta_0$, $beta_1$, and the variance of $\varepsilon \sim N(0, \sigma^2)$ is enough to characterize the relationship between vote intention and support for the statusquo.

We know that a model like this could be estimated by ordinary least squares (OLS). But we can also estimate this via maximum likelihood estimation (MLE). This is because, from the parametric models perspective, finding parameters that maximize the log likelihood is very similar to finding the parameters that minimize the sum of squared residuals. In other words, OLS regression is a special case of maximum likelihood estimation.

For your reference, the code below is how you would program MLE by hand for this problem.

```{r, eval = FALSE}
ols_ll = function(theta, y, X){
  betas = theta [1:ncol(X)]
  sigma2 = theta[ncol(X) + 1]
  mu = X %*% betas
  n = length(y)
  sigma_mat = sigma2 * diag(n)
  # Next part needs the mvtnorm package
  neg_log_likelihood = - dmvnorm(t(y), 
                                 mu, 
                                 sigma_mat,
                                 log = TRUE)
  
  return(neg_log_likelihood)
}

y = chile$yes

X = model.matrix(~statusquo, data = chile)

starting_values = c(0, 0, 1)
# one for each beta and last one for residual variance

# Try one time
ols_ll(y = y,
       X = X,
       theta = starting_values)

# This takes a while to run
# Go back to this code chunk's options
# and replace "eval = FALSE" with "cache = TRUE"
mle = optim(
  par = starting_values,
  y = y,
  X = X,
  fn = ols_ll,
  hessian = FALSE, 
  method = "BFGS",
  # In practice you would want around maxit = 5000
  # to make sure model converges
  control = list(trace = TRUE, maxit = 10)
      )

# Final log-likelihood
mle$value

# final set of parameters
# beta0, beta1, sigma^2
mle$par
```

Of course, there is a pre-packaged routine to estimate simple MLE problems in R. We use the `glm` function.

```{r}
linear_mle = glm(yes ~ statusquo, 
              data = chile, 
              family = gaussian)

coef(linear_mle)

logLik(linear_mle)
```

Which, to confirm, should be the same as estimating this model using the least squares method from last week.

```{r}
linear_ols = lm(yes ~ statusquo,
                data = chile)

coef(linear_ols)

logLik(linear_ols)
```

## Explain every step in the `ols_ll` function

## Explain every step in the `optim` function used to create the `mle` object

## Are the coefficients in `mle` the same as those in `linear_mle` and `linear_ols`? Regardless of your answer, why do you think doing MLE by hand may yield different results?

# Logistic regression

Since our outcome `yes` is binary, you may want to consider an estimation procedure that explicitly models probabilities. 

Consider the logistic regression model:

$$
p(X) = \beta_0 + \beta_1\mathtt{statusquo}
$$

Where $p(X) = Pr(Y = 1 | \mathtt{statusquo)}$. 

We can estimate this model via ML as well by changing the `family` argument to `binomial`, which by default using the logistic function as the link.

```{r}
logit0 = glm(yes ~ statusquo, 
              data = chile, 
              family = binomial) # logit link by default

coef(logit0)
```


You may notice we get very different numbers than with the classical linear model. This is because logistic regression coefficients are expressed in log odds-ratios, a byproduct of streamlining the estimation procedure.

```{r}
plot_predictions(model = logit0,
                 condition = "statusquo",
                 type = "link") +
  labs(y = expression(paste(log, 
                            bgroup("(", frac(p(X),
                                             1-p(X)),
                                   ")"))))
```

So we can interpret our result as the "effect" of one unit increase in `statusquo` the log odds ratio of `yes`.

Since this is a weird quantity, we can reverse engineer the quantity to express it in terms of probability.

```{r}
# HERE CONVERT TO PROBS
```


<!-- Log odds linear but probs not linear -->

```{r}
# "response" is the default type for logit models
plot_predictions(model = logit0,
                 condition = "statusquo",
                 type = "response") +
  labs(y = expression(p(X)))
```



# Inference