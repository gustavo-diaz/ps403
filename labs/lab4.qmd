---
title: "Lab 4: Controlling for covariates"
author: 
  - FirstName LastName
  - <YourEmail@u.northwestern.edu>
date: October 28, 2024
date-format: "**[D]ue:** MMMM D, YYYY"
format: 
     pdf:
       documentclass: article
fontsize: 12pt
urlcolor: blue
number-sections: true
geometry:
      - left=1in
      - right=1in
      - top=1in
      - bottom=1in
header-includes:
    - \usepackage{setspace}
    - \doublespacing
---

```{r setup, include=FALSE}
# Code chunk options
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE,
                      message = FALSE,
                      tidy = FALSE,
                      tidy.opts=list(width.cutoff=80))

# Packages
library(tidyverse)
library(broom) # tidyverse-friendly regression
library(rsample) # tidyverse-friendly bootstrap
library(estimatr) # easy robust standard errors
library(marginaleffects) # Visualize regression output
library(ggdist) # Shortcut for ribbon plots
library(distributional) # Shortcut for plotting regression CIs
library(modelsummary) # make pdf regression tables

```


# Data

For this lab, we will follow the application on Section 4.4 in the AM textbook.

They use the [Quality of Governance data](https://www.gu.se/en/quality-government/qog-data/data-downloads/standard-dataset) to examine the relationship between access to clean water and infant morality.

For the lab, we will use the most recent version of the Cross-Section data, which we load in the code chunk below.

```{r, results = "hide"}
url = 'https://www.qogdata.pol.gu.se/data/qog_std_cs_jan24.csv'

qog = read.csv(url)

# examine but don't print in pdf
qog
```

This is the January 2024 version of the indicators, the AM book uses an older version, so our results will differ.

This data set has 194 observations (countries) and 1652 variables. This is too many variables to carry along, so we will focus on only three.

These are:

- `wdi_mortinf`: Infant mortality rate (number of infants dying before reaching one year of age per 1,000 births in a given year)
- `who_dwtot`: % of the population with access to clean drinking water
- `wdi_acel`: % of the population with access to electricity

Infant mortality is our outcome variable. Access to clean water and electricity are our explanatory variables. I prefer these names over the traditional "dependent" and "independent" variable names because I find the similarity confusing.

The code chunk below creates a smaller version of the data that includes our outcome and explanatory variables only. We also given them more informative names.

```{r}
qog_am = qog %>% 
  select(mortinf = wdi_mortinf,
         water = who_dwtot,
         elec = wdi_acel) %>% 
  drop_na()

# examine
head(qog_am)
```

Notice that we are also dropping missing data, primarily on the `water` variable, so we are left with only 123 countries in the end.

Visualizing the data before estimating a model is generally a good idea. The code below plots the values of access to clean water in the horizontal axis against infant mortality in the vertical axis. Notice how I force a line skip in the label of the vertical axis with the `\n` expression.

```{r}
ggplot(qog_am) +
  aes(x = water, y = mortinf) +
  geom_point() +
  labs(
    x = "% Access to Clean Water",
    y = "Infant Mortality\n(per 1,000 births)"
  )
```

## How does this plot compare to Figure 4.4.1 (p. 171) in the AM textbook

## What do we need to assume to conduct statistical inference with this data? Is that assumption credible?

# Regression with one explanatory variable

We are mainly interested in the relationship between access to clean water and infant mortality. The AM book is wary about referring to this as the "effect" of access to clean water on infant mortality since it implies causation, and we are yet to talk about that. I am okay with using "effect" informally since "slope" or "derivative" are not super fun words.

We can estimate OLS regression with the `lm()` function in base R.

```{r}
fit0 = lm(mortinf ~ water, data = qog_am)
```

The traditional way to print regression output in R is with the `summary` function.

```{r, results = "hide"}
# Look at it but don't print in pdf
summary(fit0)
```

But this is hard to read and has information that we will not cover in this course. We will prefer the tidy output from the `broom` package.


```{r}
tidy(fit0)
```

You may consider that a linear specification does not fit the data well. We can introduce a quadratic fit.

```{r}
fit1 = lm(mortinf ~ water + I(water^2), data = qog_am)

tidy(fit1)
```

We can also visualize our results in ggplot. The [`marginaleffects`](https://marginaleffects.com) package provides a wrapper function that lets you plot regression objects directly.

```{r}
plot_predictions(fit0,
                 condition = "water",
                 points = 0.5) +
    labs(
      title = "Linear fit",
      x = "% Access to Clean Water",
      y = "Infant Mortality\n(per 1,000 births)"
  )
```

```{r}
plot_predictions(fit1,
                 condition = "water",
                 points = 0.5) +
    labs(
      title = "Quadratic fit",
      x = "% Access to Clean Water",
      y = "Infant Mortality\n(per 1,000 births)"
  )
```

Plotting the same lines side by side is a little bit more involved, but not impossible. The key is to extract the data from our plots.

```{r, results = "hide"}
# draw = FALSE returns a data frame instead of a ggplot object
p0 = plot_predictions(fit0,
                 condition = "water",
                 draw = FALSE)

p1 = plot_predictions(fit1,
                 condition = "water",
                 draw = FALSE)

# Inspect but do not print
p0
p1

# Paste together and add a variable to label them
pred_df = bind_rows(
  p0 %>% mutate(fit = "Linear"),
  p1 %>% mutate(fit = "Quadratic")
)


```

Then we can make feed these directly into `ggplot`. This time we will add a "rug" to indicate where the raw data is without crowding the visual space. We are also using the shortcuts from the `ggdist` and `distributional` packages to plot lines and confidence intervals.

```{r}
ggplot(qog_am) +
  aes(x = water, y = mortinf) +
  geom_rug() +
  # This part needs ggdist and distributional
  stat_lineribbon(
    data = pred_df,
    aes(x = water,
        ydist = dist_normal(mu = estimate,
                            sigma = std.error),
        fill = fit),
    alpha = 0.3
  ) +
  labs(
    x = "% Access to Clean Water",
    y = "Infant Mortality\n(per 1,000 births)"
  )
```


## Interpret the output of `tidy(fit0)`, with special attention to the `estimate` and `std.error` of each `term`. How does this compare to Table 4.4.5 in the AM book?

## Looking at the figures, which specification do you think fits the data better? Which one uses a more `efficient` estimator?

::: {.callout-tip}
## Hints

1. Recall the definition of efficiency from AM chapters 3 and 5.

2. The `deviance()` function will return the residual sum of squares from an `lm` object

:::

## Compare `fit0` and `fit1`. What happens to the `estimate` and `std.error` of `water` as we move from a linear to a quadratic specification? What statistical property or phenomena is this illustrating?


::: {.callout-tip}
## Hint

You gain something but lose something else. You could call it a *tradeoff*.

:::

# Regression with multiple covariates

Why does access to clean water matter for infant mortality? Let's pretend that our preferred explanation is that having clean drinking water helps babies avoid disease.

Someone else could argue clean water is just measuring a state's ability to promote basic infrastructure in general. There is nothing in particular about clean drinking water that keeping babies healthier, it is just a proxy for state capacity.

If the state capacity critique is true, then perhaps including a variable that is better proxy of state capacity in our regression model may change the observed coefficient for our `water` variable. Let's pretend that access to electricity `elec` is such a variable (there could be a better candidate, but I am trying to off what AM Chapter 4 does).

More formally, if `mortinf` is our outcome variable, and `water` is our explanatory variable. Then we would say that `elec` is a *control variable* or a *covariate*. I have a slight preference for the term "covariate" because it reminds you that the reason you include it in a regression model is because you expect it to *co-vary* with the outcome variable, the explanatory variable, or both.^[You could have more than one explanatory variables, but that does not automatically imply you should include both in a regression model.]


::: {.callout-note}
You will discuss this in more detail in POLI_SCI 405 or a similar course on linear models. But the importance of distinguishing between **explanatory** variables and **covariates** is that, from a substantive perspective, statistical inference in multiple regression only makes sense when we want to use covariates to **adjust** the coefficient of an explanatory variable. This usually requires (at least) a proto-causal story.
:::

Digression aside, we can include `elec` as a control variable in our regression model. To start, let's assume that a linear specification for `water` is the most appropriate, feel free to change this in the code chunk below.

```{r}
fit2 = lm(mortinf ~ water + elec, data = qog_am)
```

A more eclectic scholar may come up with a more conciliatory argument: The "effect" of `water` as a disease-preventing factor depends on other state-capacity factors captured by `elec`. After all, if you have access to clean drinking water *and* electricity, that means that you can cook nicer, keep food and antibiotics in the fridge, and perhaps other more substantial factors beyond the instructors' expertise.

If this was true, we would say that `elec` *moderates* the effect of `water`, which we can model by introducing an interaction term in the regression.

```{r}
fit3 = lm(mortinf ~ water * elec, data = qog_am)
```

If you inspect the output of `fit3`, you would notice it is the same as:

```{r}
fit3b = lm(mortinf ~ water + elec + water:elec,
           data = qog_am)
```

That is because in R formulae, the operator `a*b` tells R to look for all the possible combinations of `a` and `b`, which includes each of them by themselves. In contrast, the `a:b` operator will instruct R to only look at the literal multiplication of the two variables in the mathematical sense. For reasons that will hopefully become relevant in a future course, you usually want to include the constitutive terms of an interaction in any kind of regression estimation, so the default behavior of the `a*b` operator is desirable.

The use of interaction terms also makes the distinction between **average marginal effects (AME)** and **marginal effects at the mean (MEM)**. Or, as the AM book calls them, **average partial derivative** and **partial derivative at the average**, respectively. You could also say something like "average individual slope" and "slope at the average." The important part is that you remember those are different things. You can read a more detailed guide on this [here](https://www.andrewheiss.com/blog/2022/05/20/marginalia/).

The **AME** is the average of the effect/slope/partial derivative evaluated at the values of each observation in the data. 

The `slopes` function in the `marginaleffects` package helps us visualize these. Since regression is just an equation with many variables, we can take the partial derivative with respect to a variable of interest and then evaluate it at every point in our data.

```{r, results = "hide"}
# Evaluate but do not print in the pdf
fit3 %>% 
  slopes(variables = "water") %>% 
  as.data.frame()
```

Since our specification for `fit3` assumes an interaction between `water` and `elec`, that also means the individual slopes of `water` depend on the values of `elec`. Note that shaded area is a fluctuating confidence interval, since different values of `water` and `elec` vary in frequency.

```{r}
# This is also from marginaleffects
plot_slopes(fit3,
            variables = "water",
            condition = "elec",
            rug = TRUE)
```


So every observation has its own slope, which may or may not vary. The **AME** is just the average of these slopes. 

```{r}
fit3 %>% 
  slopes(variables = "water") %>% 
  as.data.frame() %>% 
  summarize(AME = mean(estimate))
```

Which you can corroborate with yet another function in `marginaleffects`.

```{r}
avg_slopes(fit3) 
```

In turn, the **MEM** is the effect/slope/partial derivative of a variable of interest when all variables (including the explanatory variable of interest) are held at their mean/average value. You can think of it as the expected change in slope in that particular variable as you move away from the average observation (which need not be a real observation).

```{r}
avg_slopes(fit3, newdata = "mean")
```

## Show and interpret the results of `fit2`, with special attention to the `estimate` and the `std.error` of `water`. What does this mean for our "clean water matters" story?

::: {.callout-tip}
## Hint

You can do this by showing the numbers on a tidy output, or with a figure. Choose the one that makes the most sense to you.
:::

## Why can't we just interpret each coefficient in `fit2` as the "effect" of each variable? 

::: {.callout-tip}
## Hint

Think about the distinction of total vs. direct effect and the idea of "holding constant" in some of the assigned readings for this week.

:::

## Show and interpret the results of `fit3`, with special attention to the `estimate` and the `std.error` of `water` and `water:elec`. What does this mean for the "effect of water depends on electricity" story?

## Calculate the AME and MEM of `water` for `fit0`, `fit1`, `fit2`, and `fit3`. When is it different? Why? Showing figures may help.

::: {.callout-tip}
## Tip

If you find they are never different, then when do you think they should be different? This is another one of those things that doesn't matter very often. But, when it matters, it matters **a lot**.

:::

# Standard errors

AM Chapter 4 discusses three types of standard errors for regression. In a different order from what is introduced in the book, these are.

1. Classical standard errors
2. Robust standard errors
3. Bootstrap standard errors

There used to be a time when computing these was a pain in the keister, but not anymore. Just to help you make sense of the underlying difference, this is how you would compute classical standard errors with matrix algebra for `fit2`.

```{r}
## Inspect every step if you are curious
# outcome
y = qog_am$mortinf

# right hand side vars with a leading column of 1s
X = model.matrix(~ water + elec, data = qog_am)

# beta coefficients
b = solve(t(X) %*% X) %*% t(X) %*% y

# residuals
ehat = y - (X %*% b)

# estimate variance
sigma2 = sum(ehat^2) / (nrow(qog_am) - length(b))

# variance-covariance matrix
vcovb = sigma2 * solve(t(X) %*% X)

# standard errors
seb = sqrt(diag(vcovb))

seb
```

Which is just what the base R `lm()` function calculates.

```{r}
tidy(fit2)$std.error
```

Meanwhile, Huber-White "sandwich" robust standard errors look like this.

```{r}
# We are recycling some of the elements in the previous chunk
bread = solve(t(X) %*% X)

# The part in the middle is an nxn matrix with
# residuals in the diagonal and
# zeroes in the off diagonal
meat = t(X) %*% diag(c(ehat))^2 %*% X

# Sandwich vcov matrix
sandwich = bread %*% meat %*% bread

# Robust SEs
sqrt(diag(sandwich))
```

We will corroborate this soon.

Luckily, we do not have to bother with any of this. The `lm_robust()` function in the `estimatr` package lets you implement many kinds of standard errors. You can read more about the different kinds [here](https://declaredesign.org/r/estimatr/articles/mathematical-notes.html).

This is how you implement classical standard errors, which is **not** the default in this package.

```{r}
fit2_classic = lm_robust(
  mortinf ~ water + elec,
  data = qog_am,
  se_type = "classical"
)

tidy(fit2_classic)
```

This is how you implement sandwich standard errors, which matches our matrix algebra above. Which is also not the default. The default in the package is `HC2`, but the default in Stata is `HC1`. What a life!

```{r}
fit2_robust = lm_robust(
  mortinf ~ water + elec,
  data = qog_am,
  se_type = "HC0"
)

tidy(fit2_robust)
```

You can also use `rsample` package to implement bootstrap standard errors. Which has the advantage of preserving the `lm_robust()` output format.

```{r}
# Remember the seed!
fit2_boot = bootstraps(data = qog_am, times = 1000)$splits %>% 
  map(~lm(mortinf ~ water + elec, data = analysis(.))) %>% 
  map(tidy) %>% 
  bind_rows(.id = "bootstrap_replicate") %>% 
  group_by(term) %>% 
  summarize(std.error = sd(estimate))

fit2_boot
```

## Explain what every step in `fit2_boot` is doing

## Which kind of standard error estimation procedure is more appropriate in theory? Explain

## Does it make a difference in practice? Explain

# Regression tables

If you look at regression tables in published articles, they usually have a particular format. We can reproduce this with the [`modelsummary`](https://modelsummary.com) package.

```{r}
# make a list of models
models = list(fit0, fit1, fit2, fit3)

modelsummary(models)
```

## Look at the [documentation](https://modelsummary.com/vignettes/modelsummary.html) for the `modelsummary` function. Customize the `models` table to make it as informative as possible. Explain what you changed and why you changed it

