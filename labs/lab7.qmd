---
title: "Lab 7: Hypothesis testing"
author: 
  - FirstName LastName
  - <YourEmail@u.northwestern.edu>
date: November 18, 2024
date-format: "**[D]ue:** MMMM D, YYYY"
format: 
     pdf:
       documentclass: article
fontsize: 12pt
urlcolor: blue
number-sections: true
geometry:
      - left=1in
      - right=1in
      - top=1in
      - bottom=1in
header-includes:
    - \usepackage{setspace}
    - \doublespacing
---

```{r setup, include=FALSE}
# Code chunk options
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE,
                      message = FALSE,
                      tidy = FALSE,
                      tidy.opts=list(width.cutoff=80))

# Packages
library(tidyverse)
library(haven) # read .dta files
library(infer) # Tidy-friendly hypothesis testing
library(estimatr) # Design-based estimators
library(ri2) # Randomization inference
library(mosaic) # One line permutation inference
library(MatchIt) # Matching algorithms
library(broom) # Tidy lm output
library(tinytable) # Print pdf tables
```

# Experiments and randomization inference

This week, we learned that random assignment allows us to point-identify the *average treatment effect* (ATE). Random assignment is analogous to assuming MCAR in the context of missing data. However, where MCAR is a heroic assumption, random assignment is something a researcher can have direct control over.

This is the reason why experiments are popular in academia, public policy, and industry.^[Experiments are also called randomized controlled trials (RCTs) in the policy world, and A/B tests in industry.] The most prominent feature of experimentation is the random assignment of units to conditions. There are many forms of random assignment that we will not get to cover this term.

Implicit to random assignment is a notion of **direct control**. There is nothing about the observed distribution of conditions that guarantees random assignment was conducted. We would normally expect treatment and control units to look similar across observed covariates as a result of random assignment, but that is a symptom, not a cause, of randomization. This means the only way to guarantee random assignment holds is the fact that we conducted the experiment ourselves or we have sufficient knowledge to confirm that a third party did so.

An important feature of random assignment is that it facilitates statistical inference using only the knowledge of how conditions were assigned to units. This is what Fisher meant by randomization being the reasoned basis for inference in the readings assigned for this week. This particular mode of inference is called **design-based causal inference** because the credibility of our conclusions follows from what we know (or assume) about the research design.^[Design-based causal inference is usually presented in opposition to model-based causal inference, which draws inferential leverage from what we are willing to assume about parametric models.]

To illustrate this point, we will use a smaller version of the data from [Gerber, Green, and Larimer (2008)](https://doi.org/10.1017/S000305540808009X). The version of the data that we will use is contained in the `social` data frame of the `qss` package (which is only available on GitHub).

```{r, results = "hide"}
# Not available on CRAN, install from GitHub
# remotes::install_github("kosukeimai/qss-package")
# library(qss)

# Attach on environment
# data(social)

# Alternative option if install_github doesn't work
load("http://gustavodiaz.org/ps403/data/social.RData")

# Explore but do not print in PDF
social
```

This is data from a field experiment conducted prior to the 2006 primary election in Michigan. Individuals within households were randomly assigned to receive different kinds of ["Get Out the Vote"](https://en.wikipedia.org/wiki/Get_out_the_vote) messages.^[That individuals within household were sampled, as opposed to just individuals, makes a difference for how randomization inference is conducted. We will ignore this for now.]

The following table shows the distribution of experimental conditions.

```{r}
conditions = social %>% 
  group_by(messages) %>% 
  tally()

conditions
```

The goal of the study was to understand the effect of social pressure of voter turnout. Individuals in each of the three treatment conditions received a postcard in the mail with a message encouraging people to vote. The list below indicates how each of the body of the postcard varied in each condition.


1. **Control:** No postcard

2. **Civic Duty:** "DO YOUR CIVIC DUTY -- VOTE!"

3. **Hawthorne:** Civic duty message + "YOU ARE BEING STUDIED!" + explanation of how voting behavior can be examined by means of public record.^[This is a reference to the psychological phenomenon called [Hawthorne effect](https://en.wikipedia.org/wiki/Hawthorne_effect).]

4. **Neighbors:** Civic duty message + household's previous voting record in 2004 + neighbors' voting record in 2004

We can start by estimating at the overall effect of receiving GOTV mail on turnout. For this, we need to recode a binary treatment indicator.

```{r}
# Also create new data object
gotv = social %>% 
  mutate(treatment = ifelse(messages == "Control",
                            0, 1))

# Check if it worked properly
with(gotv, table(messages, treatment))
```

We can estimate the ATE with the `difference_in_means` function in the `estimatr` package.

```{r}
ate = difference_in_means(primary2006 ~ treatment,
                    data = gotv)

tidy(ate)
```

We can also estimate the ATE with the `t.test` function (and `t_test` in the `infer` package), but notice the inversion in sign, which happens because groups are ordered differently. We will see how to control this behavior in the next section.

```{r}
t.test(primary2006 ~ treatment, 
       data = gotv)
```

Either approach gives us a non-parametric estimate (we are just comparing means) and a normal-approximation p-value. We can also compute a nonparametric p-value via randomization inference.

```{r, results = "hide", eval = FALSE}
# Set seed!
# Turn code chunk option from eval = FALSE to cache = TRUE
# This store code chunk results
# And helps you avoid having to wait for pdf every time
gotv_permutation = gotv %>% 
  specify(primary2006 ~ treatment) %>% 
  hypothesize(null = "independence") %>% 
  generate(reps = 1000, type = "permute") %>% 
  split(.$replicate) %>% 
  map(~difference_in_means(primary2006 ~ treatment, data = .)) %>% 
  map(tidy) %>% 
  bind_rows(.id = "replicate")

# Inspect
gotv_permutation
```

Then we can compute p-values as the proportion of simulated estimates that are equal or more extreme (in absolute terms) than what we observe.

```{r, error = TRUE}
mean(abs(gotv_permutation$estimate) >= abs(ate$coefficients[1]))
```

Luckily, we do not need to write such long code to conduct randomization inference, thanks to the `ri2` package. The syntax is similar to the `difference_in_means` function, but we also need to supply additional information about the experimental design.

```{r, eval = FALSE}
# Set seed and change eval = FALSE to cache =  TRUE
# Declare random assignment
gotv_assignment = declare_ra(
  N = nrow(gotv), # total units
  m = sum(gotv$treatment == 1) # total treated units
)

# Then conduct randomization inference
# I think this only works if treatment is a binary indicator
gotv_ri = conduct_ri(
  primary2006 ~ treatment,
  declaration = gotv_assignment,
  assignment = "treatment",
  data = gotv
)

```

Finally, we can also implement randomization inference, and permutation testing in general, with the `mosaic` package. Notice the use of the `shuffle()` function.

```{r, eval = FALSE}
# Set seed and change eval = FALSE to cache = TRUE
gotv_mosaic = do(1000) * diff(mean(primary2006 ~ shuffle(treatment),
                                   data = gotv))

mean(abs(gotv_mosaic) >= abs(ate$coefficients[1]))
```

We can compare these with the normal-approximation p-value.

```{r}
ate$p.value %>% round(100)
```

Next, we may wonder which of the messages was more effective relative to the control group, which would entail comparing many means simultaneously. Luckily, we can rewrite the difference-in-means as a regression estimator, which in turn means we can use `lm` or `lm_robust`.

```{r}
# Make sure control group is the reference category
gotv$messages = fct_relevel(gotv$messages,
                            "Control")

ate_lm1 = lm(primary2006 ~ messages,
                 data = gotv)

tidy(ate_lm1)
```

In this case they are not very different, but sometimes `lm` and `lm_robust` give very different results with binary outcomes because of how they handle standard errors.

```{r}
ate_lm2 = lm_robust(primary2006 ~ messages,
                      data = gotv)

tidy(ate_lm2)
```

What if you wanted to compare treatment between groups, instead of each group against the control baseline? You can adapt your `lm_robust` code to conduct *linear hypotheses* using `lh_robust`. This is equivalent to subsetting the data and then comparing the mean of each group with `diference_in_means`.

```{r}
ate_lh = lh_robust(primary2006 ~ messages,
                      data = gotv,
          linear_hypothesis = "messagesHawthorne = messagesNeighbors")

ate_lh$lh %>% tidy()
```





## Explain how every variable (other than `messages`) in the `social` data frame is measured.

## Customize the `conditions` table above to make it more informative (and show it around here).

## Interpret the result of `ate` in terms of the estimate, confidence intervals, and p-value (show a nice table if you can)

## How and why is the result using `t.test` different from using `difference_in_means`? Which one is more appropriate?


## Does randomization inference change our conclusions about `ate`? Why? Under what circumstances would you give the opposite answer?

## Compare all possible pairwise combinations of treatment groups using `lh_robust` (or any other equivalent approach). Is there a particular GOTV message that is more effective than others?

::: {.callout-tip}
## Hint

Remember to interpret estimates and p-values to inform your answer.

:::


## Compute randomization inference p-values for `ate_lm` and interpret the results. What does change relative to normal-approximation p-values?

::: {.callout-tip}
## Hint

The approach in `gotv_mosaic` may be the easiest to adapt. See [here](https://cran.r-project.org/web/packages/mosaic/vignettes/Resampling.html) to get some ideas.

:::

## Explain, to the best of your ability, what a p-value is and what they mean for statistical inference.



# Confidence intervals as inverted hypothesis tests

The Keele, McConnaughy, and White (2012) article assigned for this week discusses how we can use p-values to calculate confidence intervals by *inverting* the (sharp) null hypothesis.

This is how you do it in code.

```{r, eval = FALSE}
## Replace eval = FALSE with cache = TRUE

# Establish hypothetical test statistics
# Easy in this case since proportions are bounded in [0,1]
hyps = seq(0, 1, by = 0.001)

# Evaluate many hypotheses at the same time
ci_invert = hyps %>% 
  map(~infer::t_test(primary2006 ~ treatment,
              x = gotv,
              order = c(1, 0), # reorder groups
              mu = .)) %>%
  bind_rows() %>% 
  mutate(
    mu = hyps,
    no_reject = p_value >= 0.05
    ) %>% 
  relocate(mu, no_reject)


ci_invert %>% 
  filter(no_reject) %>% 
  summarize(estimate = mean(estimate),
            conf.low = min(mu),
            conf.high = max(mu))
```

And then compare with:

```{r}
tidy(ate) %>% select(estimate, conf.low, conf.high)
```


## Explain what the `ci_invert` object is and every step that goes into creating it.

## Explain conceptually what it means to form confidence intervals by inverting the null hypothesis. How do we interpret confidence intervals as inverted hypothesis tests?  How are they different from normal-approximation or bootstrap confidence intervals? Does it make a difference in practice? Comparing them in a table may help.




# Causal inference in observational studies

An observational study is a study in which the researcher collects or observes data from the world, but does not intervene in it as we would do in an experiment (recall the idea of *direct control*). This usually means than random assignment is not guaranteed, so the only assumption available to conduct valid causal inference is **strong ignorability.**^[You can always assume random assignment when it is not guaranteed at your own peril.]

Many of the strategies available to deal with missing data under MAR apply to estimate treatment effects under ignorability. Because of the similarity, we will return to the data from [Dettman, Pepinsky and Pierskalla's (2017)](https://doi.org/10.1016/j.electstud.2017.06.002) study on incumbency advantage in Indonesia's 2014 legislative elections.

```{r, results = "hide"}
url = "http://gustavodiaz.org/ps403/data/dpp_replicate.dta"

dpp = read_dta(url)

# drop nas just for convenience
dpp = dpp %>% drop_na

# inspect but do not print
dpp
```

As a baseline, we can estimate the effect of `incumbency` on a candidate's vote share `pct_suara` without conditioning on any covariate. We will ignore fixed effects and clustered standard errors throughout for the sake of convenience.

```{r}
baseline = lm_robust(
  pct_suara ~ incumbency,
  data = dpp
)
```

<!-- Regression -->
As we did last week, if we assume strong ignorability, we can estimate the average treatment effect by including covariates in our regression. We will ignore the `type_*` variables because there is too many

```{r}
regression = lm_robust(
  pct_suara ~ incumbency + female + age +
    factor(religion_new) +
    factor(party) + education,
  data = dpp
)
```



<!-- Matching -->
Another option is to condition on propensity scores. One option is to find a match for each observation based on how close their propensity score matches are. The `MatchIt` package makes this straightforward.

```{r}
# Set the seed in case there are ties
ps_matching = matchit(
  incumbency ~ female + age + 
    factor(religion_new) +
    factor(party) + education,
  data = dpp,
  method = "nearest", # nearest neighbor matching
  distance = "glm" # propensity score matching
)
```

There are a lot of considerations about matching that we will not discuss in this course. For now, you should know that people usually want treatment and control groups to be balanced (i.e. similar on average) across the covariates used for matching, and that matching methods can get very complicated so that we can maximize balance without sacrificing other statistical properties (e.g. [King and Nielsen 2019](https://doi.org/10.1017/pan.2019.11)) . You can read more about matching [here](https://cran.r-project.org/web/packages/MatchIt/vignettes/MatchIt.html#estimating-the-treatment-effect) and [here](https://kosukeimai.github.io/MatchIt/).

If balance is poor, then matching may not the best strategy.

```{r, fig.height=10}
match_plot = plot(summary(ps_matching))
```

Once we are happy with our matching strategy, we can extract the resulting data and estimate effects.

```{r}
dpp_match = match.data(ps_matching)

matching = lm_robust(
  pct_suara ~ incumbency,
  data = dpp_match
)
```




<!-- Propensity score  -->
Another option is to model the propensity score so that we can use it directly in IPW estimation.

```{r}
# Model propensity score
pscore_model = glm(
  incumbency ~ female + age + 
    factor(religion_new) +
    factor(party) + education,
  data = dpp,
  family = binomial)

dpp$pscore = predict(pscore_model, type = "response")

dpp$ipw = 1/dpp$pscore
```

<!-- IPW estimation without controls -->

```{r}
# Estimate
ipw0 = lm_robust(
  pct_suara ~ incumbency,
  data = dpp,
  weights = ipw
)
```


<!-- Check for overlap -->

One of the perils of conditioning on propensity scores is the problem of **overlap**. We should check how the distribution of propensity scores between groups (make the plot look prettier if you can).

```{r}
ggplot(dpp) +
  aes(x = pscore, color = factor(incumbency)) +
  geom_density()
```

It looks like we have decent overlap, but some observations have very small weights. If this is a problem, we can use *winsorized* propensity scores.

```{r}
dpp$pscore_w = case_when(
  dpp$pscore <= 0.05 ~ 0.05,
  dpp$pscore >= 0.95 ~ 0.95,
  TRUE ~ dpp$pscore
)

dpp$ipw_w = 1/dpp$pscore_w
```

Then we can estimate again using weights based on the winsorized propensity score.

```{r}
ipw1 = lm_robust(
  pct_suara ~ incumbency,
  data = dpp,
  weights = ipw_w
)
```

<!-- Doubly robust -->
Finally, we can combine both regression adjustment and IPW for a doubly robust estimator. The following code uses our default `ipw`, but you should feel free to change to the winsorized weights if you think they make more sense.

```{r}
doubly_robust = lm_robust(
  pct_suara ~ incumbency + female + age +
    factor(religion_new) +
    factor(party) + education,
  data = dpp,
  weights = ipw
)
```


## Do you believe the strong ignorability assumption makes sense in this application? Explain.

## Based on how `match_plot` looks like. Do you think matching is advisable in this application?

## Explain what overlap means and why it can pose problems when we want to estimate the ATE in observational studies.

## What do we mean when we say we *winsorized* the propensity score? Why do we do this?


## Make a table that compares the results of all the strategies we used to estimate the ATE under the strong ignorability assumption (if you can, spend some time customizing the table). What do you learn about the effect of `incumbency` on candidates vote share `pct_suara` (pay special attention to estimates and p-values)? Explain the merits and limitations of each strategy, indicate which is your preferred strategy, and explain why.

