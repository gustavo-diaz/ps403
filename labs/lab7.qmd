---
title: "Lab 7: Maximum likelihood estimation"
author: 
  - FirstName LastName
  - <YourEmail@u.northwestern.edu>
date: November 10, 2025
date-format: "**[D]ue:** MMMM D, YYYY"
format: 
     pdf:
       documentclass: article
fontsize: 12pt
urlcolor: blue
number-sections: true
geometry:
      - left=1in
      - right=1in
      - top=1in
      - bottom=1in
header-includes:
    - \usepackage{setspace}
    - \doublespacing
---

```{r setup, include=FALSE}
# Code chunk options
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE,
                      message = FALSE,
                      tidy = FALSE,
                      tidy.opts=list(width.cutoff=80))

# Packages
library(tidyverse)
library(marginaleffects) # Visualize regression output
library(modelsummary) # make pdf regression tables
library(broom) # tidy regression output
library(car) # Data for the lab
library(mvtnorm) # a package for the multivariate normal distribution
library(sandwich) # robust standard errors

```


# Data

The `Chile` data frame contains observations from a national survey conducted in April and May of 1988 by FLACSO/Chile. This survey happened six months before a plebiscite that would decide the faith of the country's dictatorship at the time. A *yes* vote would have led to eight more years of military rule, a *no* vote would set in motion a democratization process (which is what happened eventually).

```{r, eval = FALSE}
# Explore but do not print in pdf
Chile
```

You can learn more about how each variable is coded by passing `?Chile` or `help(Chile)` to the console.

Our outcome variable is `vote`, which we will recode as a binary outcome called `yes`, taking a value of 1 if the person indicated they will vote in favor of the dictatorship in the plebiscite, and 0 otherwise.

```{r}
chile = Chile %>% drop_na

chile$yes = ifelse(chile$vote == "Y", 1, 0)

# check
with(chile, table(vote, yes))
```

Our main explanatory variable will be `statusquo`, which is an aggregate index of many questions that ask the respondent whether they agree with political, social, and economic policies implemented by the military regime. Higher scores represent more support for the dictatorship's policies.

```{r}
summary(chile$statusquo)
```

## Explain what the other variables in the original `Chile` data frame are measuring

# Classical linear model

Consider the following model:

$$
\mathtt{yes} = \beta_0 + \beta_1 \mathtt{statusquo} + \varepsilon
$$

This a parametric linear model, so knowing $\beta_0$, $\beta_1$, and the variance of $\varepsilon \sim N(0, \sigma^2)$ is enough to characterize the relationship between vote intention and support for the status quo.

We know that a model like this could be estimated by ordinary least squares (OLS). But we can also estimate it via maximum likelihood estimation (MLE). This is because, from the parametric models perspective, finding parameters that maximize the log likelihood is very similar to finding the parameters that minimize the sum of squared residuals. In other words, OLS regression is a special case of maximum likelihood estimation.

For your reference, the code below is how you would program MLE by hand for this problem.

```{r, eval = FALSE}
ols_ll = function(theta, y, X){
  betas = theta [1:ncol(X)]
  sigma2 = theta[ncol(X) + 1]
  mu = X %*% betas
  n = length(y)
  sigma_mat = sigma2 * diag(n)
  # Next part needs the mvtnorm package
  neg_log_likelihood = - dmvnorm(t(y), 
                                 mu, 
                                 sigma_mat,
                                 log = TRUE)
  
  return(neg_log_likelihood)
}

y = chile$yes

X = model.matrix(~statusquo, data = chile)

starting_values = c(0, 0, 1)
# one for each beta and last one for residual variance

# Try one time
ols_ll(y = y,
       X = X,
       theta = starting_values)

# This takes a while to run
# Go back to this code chunk's options
# and replace "eval = FALSE" with "cache = TRUE"
mle = optim(
  par = starting_values,
  y = y,
  X = X,
  fn = ols_ll,
  hessian = FALSE, 
  method = "BFGS",
  # In practice you would want around maxit = 5000
  # to make sure model converges
  control = list(trace = TRUE, maxit = 10)
      )

# Final log-likelihood
mle$value

# final set of parameters
# beta0, beta1, sigma^2
mle$par
```

Of course, there is a pre-packaged routine to estimate simple MLE problems in R. We use the `glm` function.

```{r}
# family = gaussian is the default for glm
# It implies assuming a multivariate normal
# distribution
# "gaussian" is a special kind of normal curve
linear_mle = glm(yes ~ statusquo, 
              data = chile, 
              family = gaussian)

coef(linear_mle)

logLik(linear_mle)
```

Which, to confirm, should be the same as estimating this model using the least squares method from last week.

```{r}
linear_ols = lm(yes ~ statusquo,
                data = chile)

coef(linear_ols)

logLik(linear_ols)
```

## Explain every step in the `ols_ll` function

## Explain every argument in the `optim` function used to create the `mle` object

## Are the coefficients in `mle` the same as those in `linear_mle` and `linear_ols`? Regardless of your answer, why do you think doing MLE by hand may yield different results?

# Logistic regression

Since our outcome `yes` is binary, you may want to consider an estimation procedure that explicitly models probabilities. 

Consider the logistic regression model:

$$
p(X) = \beta_0 + \beta_1\mathtt{statusquo}
$$

Where $p(X) = Pr(\mathtt{yes} = 1 | \mathtt{statusquo)}$. 

We can estimate this model via ML as well by changing the `family` argument to `binomial`, which by default uses the logistic function as the link.

```{r}
logit0 = glm(yes ~ statusquo, 
              data = chile, 
              family = binomial) # logit link

coef(logit0)
```


You may notice we get very different numbers than with the classical linear model. This is because logistic regression coefficients are expressed in log odds ratios (log odds for short), a byproduct of streamlining the estimation procedure.

Visualizing this in a `marginaleffects` plot can help us interpret these estimates.

```{r}
plot_predictions(model = logit0,
                 condition = "statusquo",
                 # Plot link function on Y axis
                 type = "link") +
  # the expression function is just to write math in the labels
  labs(y = expression(paste(log, 
                            bgroup("(", frac(p(X),
                                             1-p(X)),
                                   ")"))))
```

Because the relationship between `statusquo` and the log odds is linear, we can interpret our result as the "effect" of one unit increase in `statusquo` on the log odds of `yes`.

Since log odds is a weird unit, we can reverse engineer the numbers to express then in terms of probability. The function below grabs a model object and converts the coefficients' unit from log odds to probabilities.

```{r}
get_logit_prob = function(fit){
  coefs = coef(fit) # extract coefficients from logit fit
  odds = exp(coefs) # transform log-odds to odds
  prob = odds / (1 + odds) # convert odds to probabilities
  prob = round(prob, 5) # round for easy interpretation
  return(prob)
}
```

We can apply this to our `logit0` model.

```{r}
get_logit_prob(logit0)
```

However, the problem is that our link function uses an s-shaped [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) to bound probabilities in the $[0,1]$ range.

Again, this is easier to visualize with a figure.

```{r}
# "response" is the default type for logit models
plot_predictions(model = logit0,
                 condition = "statusquo",
                 type = "response") +
  labs(y = expression(p(X)))
```

Because the relationship between `statusquo` and the predicted probabilities is not in a linear scale, converting the coefficients to probabilities does not give you much information. The slope (or derivative) of the curve depends on the values of `statusquo`, so the only thing you can conclude by looking at the coefficients of a logistic regression model is whether the curve is generally increasing or decreasing, and you can already convey that information using log odds as the scale.^[Converting log odds coefficients into probabilities may still be helpful when you have binary or categorical predictors, since they convey all the information about the predictors at their relevant values.]

## Using one or more figures, compare the results of `linear_mle` and `logit_0`. Which model seems to be a better fit for the data? Which is more plausible to assume as the true model from a parametric perspective?

:::{.callout-tip}
The slides for this week have an example of how to plot two models together using `marginaleffects`. You could also visualize one model at a time with the `plot_predictions` function in the same package.
:::

## Look at the log likelihood for each model. Does it change your answer to the previous question? Explain

## Propose an alternative model specification for `logit_0` that includes at least one additional control variable. Estimate this model with logistic regression. Show the coefficients and make a figure to compare the results with `linear_mle` and `logit_0`. How does the model you propose change our interpretation of the relationship between `statusquo` and voting `yes`?

::: {.callout-tip}
## Tips

1. You can also use the code in the slides for this week to plot the three models together with `marginaleffects` functions. You need to adapt it to visualize the results of three models now.

2. You could include as many new variables as you want to, but remember Achen's ART mantra.
:::

# Inference

A nice feature of parametric models is that, if we assume the model is true, we can also safely assume that the asymptotic properties required to justify the use of classical normal-approximation standard errors are also true. 

Similar to how `lm` works, `glm` produces classical standard errors by default.

```{r}
tidy(logit0)
```

However, unlike `lm` in base R or `lm_robust` in the `estimatr` package, we do not get confidence intervals unless we ask for them.

```{r}
confint(logit0)
```


From a nonparametric approach, classical normal-approximation standard errors required somewhat heroic assumptions. However, from the parametric point of view, you need a *very* strong reason to use anything other than classical standard errors.

AM Chapter 5 gives us one general reason. In some applications, MLE may serve as a decent approximation even if the underlying model is wrong. In some cases, it may be better to use a well known model and "tweak" its standard errors instead of rewriting a whole new model. This would be an example of a semiparametric model. We are keeping the parametric portion that estimates betas in the original model but then we add a nonparametric layer to calculate standard errors, meaning we do not assume anything in particular about their distribution.

Still, nothing stops you from calculating robust standard errors in a logit model. There is no one-line routine like in `lm_robust`, but you can use the [`sandwich`](https://sandwich.r-forge.r-project.org/index.html) package to compute robust standard errors in any kind of `glm` model object.

```{r}
# Get variance-covariance matrix
logit_vcov = vcovHC(logit0, type = "HC0")

# the square root of the diagonal give our
# standard errors
logit_se = sqrt(diag(logit_vcov))

logit_se
```

Which you can compare with:

```{r}
tidy(logit0) %>% .$std.error
```

Ultimately, this is a matter of open debate. From a *pure* parametric models perspective, nothing about logistic regression requires us to assume anything about how errors are distributed (see [here](https://davegiles.blogspot.com/2013/05/robust-standard-errors-for-nonlinear.html) and [here](https://statmodeling.stat.columbia.edu/2017/12/27/id-say-robust-standard-errors-like-holy-roman-empire-except-think-theyre-error/) for poignant arguments). [King and Roberts (2015)](https://doi.org/10.1093/pan/mpu015) even argue that the divergence of classical and robust standard errors is a symptom of using the wrong model. [Aronow (2016)](https://doi.org/10.48550/arXiv.1609.01774) retorts by suggesting that using robust standard errors helps us recast parametric models as semiparametric, which in turn requires us to assume fewer things about the data generation process, and fewer assumptions implies more credibility (in the agnostic statistics sense).

## What do you think? Do you see yourself using robust standard errors in logistic regression or another kind of parametric model? How would you justify your choice during a job talk?

## What about bootstrapped standard errors? Do they make sense for parametric models conceptually? 

## Compute bootstrap standard errors for `logit_0`. How do they compare to classical and robust standard errors?

::: {.callout-tip}
## Hint

This code worked well for OLS regression last week, it may also work fine for logit.

```{r, eval = FALSE}
library(rsample)

# Remember the seed!
fit2_boot = bootstraps(data = qog_am, times = 1000)$splits %>% 
  map(~lm(mortinf ~ water + elec, data = analysis(.))) %>% 
  map(tidy) %>% 
  bind_rows(.id = "bootstrap_replicate") %>% 
  group_by(term) %>% 
  summarize(std.error = sd(estimate))
```

:::

## Calculate confidence intervals for the estimate of `statusquo` in `logit_0` using classical, robust, and bootstrap standard errors. Does our choice or standard error affect the inferences that we are able to draw about the relationship between voting `yes` and `statusquo`? Why do you think that is the case?




