---
title: "Lab 6: Dealing with Missing Data"
author: 
  - FirstName LastName
  - <YourEmail@u.northwestern.edu>
date: November 11, 2024
date-format: "**[D]ue:** MMMM D, YYYY"
format: 
     pdf:
       documentclass: article
fontsize: 12pt
urlcolor: blue
number-sections: true
geometry:
      - left=1in
      - right=1in
      - top=1in
      - bottom=1in
header-includes:
    - \usepackage{setspace}
    - \doublespacing
---

```{r setup, include=FALSE}
# Code chunk options
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE,
                      message = FALSE,
                      tidy = FALSE,
                      tidy.opts=list(width.cutoff=80))

# Packages
library(tidyverse)
library(estimatr) # Regression with robust standard errors
library(broom) # Tidy lm output
library(tinytable) # Print pdf tables
library(haven) # Read .dta files
library(car) # Data for the lab
library(mice) # Multivariate Imputation by Chained Equations
library(Amelia) # Multiple imputation via bootstrapping and EM algorithm
```

# Missing not at random

This week, we learned about two assumptions that one could make to conduct *valid* statistical inference with missing data, MCAR and MAR. Both are restrictive assumptions and, in most applied settings, we can only justify observations Missing Not At Random (MNAR).^[Sometimes also called Not Missing at Random (NMAR). MNAR and NMAR mean the same, the only difference being grammatical preference.] MNAR implies that data are missing in a way that depends on unobserved covariates, so we cannot simply adjust or impute based on the data we have collected.

Still, we make assumptions not because they are true, but because they are useful. From that point of view, you want to show that whatever you assume is not particularly harmful for the inferences you draw.

To illustrate this point, consider the study by [Dettman, Pepinsky and Pierskalla (2017)](https://doi.org/10.1016/j.electstud.2017.06.002) on incumbency advantage in Indonesia's 2014 legislative elections.

To begin, we will load the data.

```{r, results = "hide"}
url = "http://gustavodiaz.org/ps403/data/dpp_replicate.dta"

dpp = read_dta(url)

# inspect but do not print
dpp
```

Each observation is a candidate. Indonesia uses open list PR, so there are many candidates but incumbents are a small proportion. The original data already has some missing observations, but we will pretend that it does not for the sake of illustration. This drops about 600 observations from the 6600 total.

```{r, results = "hide"}
# drop nas
dpp = dpp %>% drop_na

# inspect but do not print
dpp
```

We will then reproduce their main finding, which is the effect of being an incumbent on a candidate's vote share. The model includes a bunch of covariates (the `type_*` variables are indicators for different kinds of candidate occupations). We are also including fixed effects for electoral districts and clustering our standard errors by electoral district as well.^[You will learn what fixed effects and clustered standard errors are in POLI_SCI 405.]

To avoid repeated copy-pasting, we will save our regression specification as a formula object.

```{r}
specification = formula(
"pct_suara ~ incumbency +
female + age + factor(religion_new) +
factor(party) + education +
type_1 + type_2 + type_3 + type_5 + type_6 + type_7 +
type_9 + type_10 + type_11 + type_12 + type_13 +
type_14 + type_15 + type_16 + type_17 + type_18 + 
type_19 + type_21"
)

lm_original = lm_robust(
  formula = specification,
  data = dpp,
  fixed_effects = ~id_dapil,
  clusters = id_dapil,
  se_type = "stata" # too slow otherwise
)

# We only need to look at the effect of incumbency
original = tidy(lm_original) %>% 
  filter(term == "incumbency") %>% 
  mutate(procedure = "Original") %>% 
  select(procedure, term, estimate, std.error)

original
```

Let's pretend this is our "ground truth."^[This is exactly the same as the result presented in Column 1 of Table 3 in the original paper. Exact reproducibility is rarely this easy.] We can introduce non-ignorable missingness by randomly selecting 25% of the observations and assigning the vote share outcome `pct_suara` to missing if the candidate's vote is in the highest quartile. We can claim this is MNAR since the source of missingness is both non-random and also not driven by any of the observed covariates.

```{r}
# Set the seed!
dpp$miss_vote = rbinom(nrow(dpp), 1, 0.25)

dpp$high_vote = dpp$pct_suara > quantile(dpp$pct_suara, 0.75)

dpp_missing = dpp %>% 
  mutate(drop = ifelse(miss_vote == 1 & high_vote, 1, 0))
```


We can check the consequences of introducing missing data by estimating the effect of `incumbency` on the resulting `dpp_missing` data frame, which is equivalent to the default *listwise deletion* in `lm` and `lm_robust`. This is what most people would do by default if they are not particularly troubled by missing data. Listwise deletion would only yield valid estimates if we assume MCAR, but people rarely invoke this assumption explicitly when letting software drop missing observations automatically.

```{r}
lm_listwise = lm_robust(
  formula = specification,
  data = dpp_missing %>% filter(drop == 0),
  fixed_effects = ~id_dapil,
  clusters = id_dapil,
  se_type = "stata" # too slow otherwise
)

listwise = tidy(lm_listwise) %>% 
  filter(term == "incumbency") %>% 
  mutate(procedure = "Listwise deletion") %>% 
  select(procedure, term, estimate, std.error)

listwise
```

A more principled approach would be to make the minimal assumption of the effect of `incumbency` being *bounded*. We can estimate lower and upper bounds by replacing our would be missing observations with the minimum and maximum values of our observed units.

```{r}
range = dpp_missing %>% 
  filter(drop == 0) %>% 
  summarize(min_vote = min(pct_suara),
            max_vote = max(pct_suara))

dpp_bound_low = dpp %>% 
    mutate(pct_suara = ifelse(miss_vote == 1 & high_vote,
                       range$min_vote, pct_suara))

dpp_bound_high = dpp %>% 
    mutate(pct_suara = ifelse(miss_vote == 1 & high_vote,
                       range$max_vote, pct_suara))


lm_bound_low = lm_robust(
  formula = specification,
  data = dpp_bound_low,
  fixed_effects = ~id_dapil,
  clusters = id_dapil,
  se_type = "stata"
)

lm_bound_high = lm_robust(
  formula = specification,
  data = dpp_bound_high,
  fixed_effects = ~id_dapil,
  clusters = id_dapil,
  se_type = "stata"
)

bound_low = tidy(lm_bound_low) %>% 
  filter(term == "incumbency") %>% 
  mutate(procedure = "Lower bound") %>% 
  select(procedure, term, estimate, std.error)

bound_high = tidy(lm_bound_high) %>% 
  filter(term == "incumbency") %>% 
  mutate(procedure = "Upper bound") %>% 
  select(procedure, term, estimate, std.error)

# Visualize together
bind_rows(bound_low, bound_high)
```


A more restrictive assumption would be to explicitly assume MCAR, in which case we can impute the mean value of the observed sample into missing outcomes. This is referred to as *mean imputation* for short.

```{r}
# Calculate avg vote share if not missing
mean_vote_missing = dpp_missing %>%
  filter(drop == 0) %>% 
  summarize(avg_vote = mean(pct_suara)) %>% 
  .$avg_vote # This last part returns the number as a vector

# Impute sample average if missing
dpp_mcar = dpp %>% 
  mutate(pct_suara = ifelse(miss_vote == 1 & high_vote,
                       mean_vote_missing, pct_suara))

# Estimate
lm_mcar = lm_robust(
  formula = specification,
  data = dpp_mcar,
  fixed_effects = ~id_dapil,
  clusters = id_dapil,
  se_type = "stata"
)

mcar = tidy(lm_mcar) %>% 
  filter(term == "incumbency") %>% 
  mutate(procedure = "MCAR imputation") %>% 
  select(procedure, term, estimate, std.error)

mcar
```

We can also consider two options that become viable if we assume MAR. First, we can impute missing values based on the predictions of a regression model, then estimate. Notice how we use the `update` function to avoid rewriting a long formula object.



```{r}
# Update specification to fit lm object
outcome_spec = update(specification,
                      ~ . + as.factor(id_dapil))

# Estimate outcome model on observed data
outcome_model = lm(outcome_spec,
                   data = dpp_missing %>% 
                     filter(drop == 0))

# Draw predictions in the whole data set
dpp$reg_pred = predict(outcome_model, newdata = dpp)

# Impute data
dpp_reg = dpp %>% 
  mutate(pct_suara = ifelse(miss_vote == 1 & high_vote,
                       reg_pred, pct_suara))

# Estimate
lm_reg = lm_robust(
  formula = specification,
  data = dpp_reg,
  fixed_effects = ~id_dapil,
  clusters = id_dapil,
  se_type = "stata"
)

regression = tidy(lm_reg) %>% 
  filter(term == "incumbency") %>% 
  mutate(procedure = "Regression imputation") %>% 
  select(procedure, term, estimate, std.error)

regression
```

Which, incidentally, is the same as just replacing the every observation with its prediction, although with nonsensical standard errors. You would need bootstrap standard errors for consistent estimation. We will not consider this alternative moving forward since we do not have a reason to think bootstrapping is worth the extra work.

```{r}
# Just for reference
specification2 = update(specification,
                        reg_pred ~ .) 

lm_robust(
  formula = specification2,
  data = dpp,
  fixed_effects = ~id_dapil,
  clusters = id_dapil,
  se_type = "stata"
) %>% 
  tidy() %>%
  filter(term == "incumbency") %>% 
  select(term, estimate, std.error)
```

Another option under the MCAR assumption is to estimate a propensity score, and then use inverse probability weighting (IPW) to adjust results. The original model already includes a regression specification with covariates, so a "pure" IPW estimation doesn't make much sense. Instead, the code below combines regression with IPW, which is equivalent to implementing *doubly robust estimation*.

```{r}
# Estimate response propensity model
# Remember drop == 1 means missing, so we need to invert
pscore_spec = update(specification,
                     (1-drop) ~ .)

pscore_model = glm(pscore_spec,
                   data = dpp_missing,
                   family = binomial)

# Predict on entire data set
pscore = predict(pscore_model, type = "response")

# Create IP weights
dpp_missing$ipw = 1/pscore

# Estimate effect
lm_dr = lm_robust(
  formula = specification,
  data = dpp_missing %>% filter(drop == 0),
  fixed_effects = ~id_dapil,
  clusters = id_dapil,
  se_type = "stata",
  weights = ipw
)

dr = tidy(lm_dr) %>% 
  filter(term == "incumbency") %>% 
  mutate(procedure = "Doubly robust estimation") %>% 
  select(procedure, term, estimate, std.error)

dr  
```

Passing the `weights` argument turns the estimation into *weighted least squares*, which is the closest analog to Hájek's stabilized IPW in the multivariate regression context. See [this note](https://declaredesign.org/r/estimatr/articles/mathematical-notes.html#lm_robust-notes) for details.

Just for your reference, this is how you would perform ipw adjustment on its own in the regression context with the `estimatr` package. It does not make sense to consider this option in this application, so we will not consider it moving forward.

```{r}
# Just for reference
lm_robust(
  formula = pct_suara ~ incumbency, # no covariates
  data = dpp_missing %>% filter(drop == 0),
  fixed_effects = ~id_dapil,
  clusters = id_dapil,
  se_type = "stata",
  weights = ipw
) %>% 
  tidy() %>% 
  select(term, estimate, std.error)
```

Finally, we can use the `tinytable` package to visualize all the results at the same time (try to make the table look more informative if you can).

```{r}
results = bind_rows(
  original,
  listwise,
  bound_low,
  bound_high,
  mcar,
  regression,
  dr
)

results %>% 
  tt() %>% 
  theme_tt("placement", latex_float = "H") # print table right after code
```


## Conceptually, what do MCAR and MAR mean? How are they different? When is one more appropriate than the other?

## Interpret the results of each procedure. How well do they approximate the original results? Are they worth the additional assumptions?

## Add confidence intervals to the table. In terms of statistical inference, how do our overall conclusions about the effect of `incumbency` change with each procedure?

## Generally speaking, what is your preferred procedure? Explain

# Multiple imputation

The previous example considered missing data in the outcome only. We will encounter data with missing values across multiple covariates often. How to deal with missing data in this context?

To illustrate the problem, consider the `UN98` data set contained in the `car` package, which contains social indicators on 207 nations distributed by the United Nations circa 1998. You can learn more by checking the documentation with `?UN98`.

We can inspect the data to see that many variables have missing observations.

```{r}
UN98 %>% 
  summarize_all(~round(mean(is.na(.x)), 2)) %>% 
  pivot_longer(region:illiteracyFemale,
              names_to = "variable",
               values_to = "prop_missing")
```


<!-- Listwise deletion -->

We are interested in the conditional expectation of `infantMortality` by `GDPperCapita`, controlling for `educationFemale` and `region`. The following estimation performs listwise deletion by default, which in the multiple imputation world is also known as *complete cases* analysis.

```{r}
lm_complete = lm(log(infantMortality) ~ log(GDPperCapita) +
                      educationFemale + region, data = UN98)

df_complete = tidy(lm_complete) %>% 
  filter(term == "log(GDPperCapita)") %>% 
  mutate(procedure = "Complete cases") %>% 
  select(procedure, term, estimate, std.error)

df_complete
```


Notice we are transforming both outcome and explanatory variable by taking the natural logarithm. We usually want to do this for variables that are theorized to have diminishing returns. This is called a [*log-log* transformation](https://library.virginia.edu/data/articles/interpreting-log-transformations-in-a-linear-model) because both outcome and explanatory variable are transformed that way. We can roughly interpret the coefficient as the average percent change in the outcome as a result of a one percent increase in the explanatory variable, holding everything else constant. So, in this application, one percent increase in `GDPperCapita` leads to a `0.27` percent **decrease** in `infantMortality`.^[Remember these are percents, not percentage points.]

More to the point, our baseline models discards a lot of data.

```{r}
glance(lm_complete)$nobs
```

We are only using 76 of out of 207 observations, so we can certainly do better. The challenge is that we have missing data all over the place, even in variables that we are not using for estimation. Therefore, we cannot rely on regression imputation or IPW since those models will also suffer from missing data.

<!-- mice -->
The solution is to address the missing data problem as part of the data *pre-processing* step. We have two popular options to do so in the social sciences, both assuming MAR across variables. The first is called [Multivariate Imputation by Chained Equations (MICE)](https://amices.org/mice/). As the name suggests, this approach iteratively fills in missing data for every variable through a bunch of back and forth predictive models. See [here](https://stefvanbuuren.name/publications/2011%20MICE%20-%20JSS.pdf) for technical details and [here](https://www.john-fox.ca/Companion/appendices/Appendix-Multiple-Imputation.pdf) for an extended tutorial.

We can implement MICE with the `mice` function in the `mice` package. There is a lot to tinker with here, but the default imputation method is called *predictive mean matching*, which is essentially a series of hot-deck imputation steps using different models depending on the type of variable (e.g. OLS if continuous, logistic regression if binary, multinomial logit if polytomous).

A default call to `mice` will return five candidate data frames with imputed observations across variables. The `UN98` data does not contain missing factors, so this is all the result of chained OLS regression models.

```{r, results = "hide", message = FALSE, warning = FALSE}
# set the seed!
mice_imp = mice(UN98) # contains 5 options
```

We can then select one of the five candidates to conduct statistical inference.

```{r, resuts = "hide"}
# Select one candidate data
UN98_mice = complete(mice_imp, 3)

# Estimate
lm_mice = lm(log(infantMortality) ~ log(GDPperCapita) +
               educationFemale + region, data = UN98_mice)

# Tidy up
df_mice = tidy(lm_mice) %>% 
  filter(term == "log(GDPperCapita)") %>% 
  mutate(procedure = "MICE") %>% 
  select(procedure, term, estimate, std.error)

df_mice
```

To confirm, we can check the effective sample size.

```{r}
glance(lm_mice)$nobs
```

<!-- amelia -->
The second option is a program called [Amelia II](https://gking.harvard.edu/amelia), named after the pioneer aviator. Amelia uses a combination of bootstrapping and the [expectation-maximization algorithm](https://en.wikipedia.org/wiki/Expectation–maximization_algorithm) to impute missing data across variables. See [here](https://gking.harvard.edu/sites/scholar.harvard.edu/files/gking/files/evil.pdf) for more a technical description.

Amelia II has a similar implementation as MICE. We use the `amelia` function in the `Amelia` package (note the difference in capitalization). We also get 5 candidate data frames by default. The main difference is that factors need to be transformed into a usable format or declared as ID variables so that they are ignored by the program. In this application, we instruct Amelia to ignore the `region` variable.


```{r, results = "hide", message = FALSE, warning = FALSE}
# Set the seed!
amelia_imp = amelia(UN98, idvars = c("region")) # also gives 5 options
```

Notice the warning about Amelia choosing to ignore observations that are completely missing, which is different from MICE. After choosing one of the five candidates, we can estimate.

```{r}
# Select one candidate data
UN98_amelia = amelia_imp$imputations$imp3

# Estimate
lm_amelia = lm(log(infantMortality) ~ log(GDPperCapita) +
                 educationFemale + region, data = UN98_amelia)

# Tidy up
df_amelia = tidy(lm_amelia) %>% 
  filter(term == "log(GDPperCapita)") %>% 
  mutate(procedure = "Amelia II") %>% 
  select(procedure, term, estimate, std.error)

df_amelia
```


We can check the effective sample under Amelia imputation. We use `r glance(lm_amelia)$nobs` out of 207 observations.

```{r}
glance(lm_amelia)$nobs
```

To facilitate interpretation, we can combine the three multiple imputation procedures into one table (you can also spend some time making this table more informative).

```{r}
results2 = bind_rows(
  df_complete,
  df_mice,
  df_amelia
)

results2 %>% 
  tt() %>% 
  theme_tt("placement", latex_float = "H")
```

## Interpret the results of each procedure. Which appears more attractive for this application? Explain

## Generally speaking, what is your preferred procedure for multiple imputation? Explain

